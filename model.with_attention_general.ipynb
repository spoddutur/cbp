{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import import_ipynb\n",
    "#from model import *\n",
    "import numpy as np\n",
    "import itertools\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.backend import mean, sum, expand_dims, tanh\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, GRU, Concatenate, Multiply, Reshape, RepeatVector, Dot, Softmax\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Lambda\n",
    "from keras.optimizers import Adam\n",
    "np.random.seed(1)\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given x (?,dim) always return (max_len, dim) by padding with zeros\n",
    "def padding(max_len, x, dim ):\n",
    "    x_len = x.shape[0]\n",
    "    # truncate if more than max_len else pad with zeros\n",
    "    if(x_len >= max_len):\n",
    "        return x[0:max_len]\n",
    "    else:   \n",
    "        padding = np.zeros((max_len-x_len, dim))     \n",
    "        if(len(x.shape) > 1):\n",
    "            assert dim == x.shape[1],\"dimensions didnt match in padding\"   \n",
    "            return np.concatenate((x, padding))\n",
    "        else:\n",
    "            return padding\n",
    "        \n",
    "# given 1D array x, we return array of shape (max_len, dim) by replicating each element dim times and apdding with zeros at end\n",
    "def reshape_mask(max_len, x, dim):\n",
    "    # mask is a 1D array so padd with dim == 1\n",
    "    padded_question_mask = np.squeeze(padding(max_len, x, 1))\n",
    "    final_question_mask = []\n",
    "    for qm in padded_question_mask:\n",
    "        # make each mask index into array of dimension dim\n",
    "        final_question_mask.append(np.array([qm]*dim))\n",
    "    return np.asarray(final_question_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s_DimOfQuestionVector = 300\n",
    "s_DimOfTripletVector  = 900\n",
    "s_DimOfMask           = 50\n",
    "s_QuestionMaxWords    = 60\n",
    "s_TripletsMaxNumbers  = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:/cs230/proj0/ned-graphs/data_wiki_encoded/train/train_8.ipwk\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(data, \n",
    "                 question_max_words, \n",
    "                 triplets_max_numbers, \n",
    "                 dimOfQuestionVector, \n",
    "                 dimOfTripletVector, \n",
    "                 dimOfMask):\n",
    "    question_vectors = []\n",
    "    question_masks = []\n",
    "    triplet_vectors = []\n",
    "    Y = []\n",
    "    for elem in data:\n",
    "        item_question             = elem[\"question\"]\n",
    "        padded_item_question      = padding(question_max_words, item_question, dimOfQuestionVector)\n",
    "        item_question_mask        = np.reshape(elem[\"question_mask\"], (len(elem[\"question_mask\"]), 1) )\n",
    "        padded_item_question_mask = reshape_mask(question_max_words, item_question_mask, dimOfMask)\n",
    "        item_correct_triplets     = elem[\"correct_triplets\"]\n",
    "        item_wrong_triplets       = elem[\"wrong_triplets\"]\n",
    "        \n",
    "        # for triplets take mean of each S, P  and 0 to get 3,300 vectors which are then concatenated to dimOfTripletVector vector\n",
    "        item_correct_triplets_vectors = [] \n",
    "        for tr in item_correct_triplets: # There are triplets_max_numbers triplets\n",
    "            # Each tr has s, p and o. Each of the _s, _p and _o's are (num_tokens, dimOfTripletVector) size\n",
    "            _s = np.mean(tr[0], axis=0)\n",
    "            _p = np.mean(tr[1], axis=0)\n",
    "            _o = np.mean(tr[2], axis=0)\n",
    "            item_correct_triplets_vectors.append(np.concatenate((_s, _p, _o))) # adding (900,) vector for each triplet\n",
    "        #pad triplet vectors so we have (triplets_max_numbers, dimOfTripletVector)  encoding for all triplets\n",
    "        padded_item_triplets_vectors = padding(triplets_max_numbers, np.asarray(item_correct_triplets_vectors), dimOfTripletVector)\n",
    "        \n",
    "        triplet_vectors.append(padded_item_triplets_vectors)\n",
    "        question_vectors.append(padded_item_question)\n",
    "        question_masks.append(padded_item_question_mask)\n",
    "        Y.append(1)\n",
    "\n",
    "        # for wrong triplets\n",
    "        item_wrong_triplets_vectors = []\n",
    "        for tr in item_wrong_triplets:\n",
    "            _s = np.mean(tr[0], axis=0)\n",
    "            _p = np.mean(tr[1], axis=0)\n",
    "            _o = np.mean(tr[2], axis=0)\n",
    "            item_wrong_triplets_vectors.append(np.concatenate((_s, _p, _o)))\n",
    "        padded_item_wrong_triplets_vectors = padding(triplets_max_numbers, np.asarray(item_wrong_triplets_vectors), dimOfTripletVector)\n",
    "                \n",
    "        triplet_vectors.append(padded_item_wrong_triplets_vectors)\n",
    "        question_vectors.append(padded_item_question)\n",
    "        question_masks.append(padded_item_question_mask)\n",
    "        Y.append(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    question_vectors = np.asarray(question_vectors)\n",
    "    question_masks   = np.asarray(question_masks)\n",
    "    triplet_vectors  = np.asarray(triplet_vectors)\n",
    "    Y                = np.asarray(Y)\n",
    "    return question_vectors, question_masks, triplet_vectors, Y\n",
    "\n",
    "def generate_model_inputs():\n",
    "    file_list = glob.glob('X:/cs230/proj0/ned-graphs/data_wiki_encoded/train/train_8.ipwk')\n",
    "    data = []\n",
    "    num = 0\n",
    "    for inputFile in file_list:\n",
    "        print(inputFile)\n",
    "        f = open(inputFile, 'rb')\n",
    "        test = pickle.load(f)\n",
    "        data = np.append(data, test)\n",
    "        f.close()\n",
    "        num += 1\n",
    "        if num == 1:\n",
    "            break\n",
    "    \n",
    "    return prepare_data(data, \n",
    "                        question_max_words   = s_QuestionMaxWords, \n",
    "                        triplets_max_numbers = s_TripletsMaxNumbers, \n",
    "                        dimOfQuestionVector  = s_DimOfQuestionVector, \n",
    "                        dimOfTripletVector   = s_DimOfTripletVector, \n",
    "                        dimOfMask            = s_DimOfMask )\n",
    "\n",
    "question_vectors, question_masks, triplet_vectors, Y = generate_model_inputs()#prepare_data(data, question_max_words=60, triplets_max_numbers=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 60, 300)\n",
      "(200, 60, 50)\n",
      "(200, 500, 900)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(question_vectors.shape)\n",
    "print(question_masks.shape)\n",
    "print(triplet_vectors.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_input_shape      = (s_QuestionMaxWords,   s_DimOfQuestionVector)\n",
    "questions_mask_input_shape = (s_QuestionMaxWords,   s_DimOfMask )\n",
    "triplets_input_shape       = (s_TripletsMaxNumbers, s_DimOfTripletVector )\n",
    "# question_vectors(, 60, 300), question_masks(, 60, 300), triplet_vectors (,500,900), Y\n",
    "\n",
    "gru_dimension = s_DimOfMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "question_vectors_input (InputLa (None, 60, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Sentence_GRU1 (GRU)             (None, 60, 50)       52650       question_vectors_input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "triplets_input (InputLayer)     (None, 500, 900)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_masks_input (InputLaye (None, 60, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Sentence_GRU2 (GRU)             (None, 60, 50)       15150       Sentence_GRU1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Dense50T (Dense)                (None, 500, 50)      45050       triplets_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 60, 50)       0           question_masks_input[0][0]       \n",
      "                                                                 Sentence_GRU2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Triplet_GRU1 (GRU)              (None, 500, 50)      15150       Dense50T[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "SentenceMean (Lambda)           (None, 50)           0           multiply_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Triplet_GRU2 (GRU)              (None, 500, 50)      15150       Triplet_GRU1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "AttentionDense (Dense)          (None, 500, 50)      2500        Triplet_GRU2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "AttentionRepeat (RepeatVector)  (None, 500, 50)      0           SentenceMean[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "AttentionDotMul (Multiply)      (None, 500, 50)      0           AttentionDense[0][0]             \n",
      "                                                                 AttentionRepeat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "AttentionDotsum (Lambda)        (None, 500)          0           AttentionDotMul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "AttentionSofMax (Softmax)       (None, 500)          0           AttentionDotsum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "AttentionWeightedSum (Lambda)   (None, 50)           0           Triplet_GRU2[0][0]               \n",
      "                                                                 AttentionSofMax[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SentenceTripletConcat (Concaten (None, 100)          0           SentenceMean[0][0]               \n",
      "                                                                 AttentionWeightedSum[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Dense64 (Dense)                 (None, 64)           6464        SentenceTripletConcat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Dense1 (Dense)                  (None, 1)            65          Dense64[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 152,179\n",
      "Trainable params: 152,179\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# question inputs\n",
    "question_vectors_input = Input(shape=questions_input_shape,      dtype='float32', name=\"question_vectors_input\")   \n",
    "question_masks_input   = Input(shape=questions_mask_input_shape, dtype='float32', name=\"question_masks_input\")\n",
    "\n",
    "# triplets input\n",
    "triplets_input = Input(shape=triplets_input_shape, dtype='float32', name=\"triplets_input\")  #(None, 500, 900)\n",
    "\n",
    "# layers\n",
    "# gru1 = Bidirectional(GRU( 50, return_sequences=True, name=\"Sentence_GRU1\"))\n",
    "# gru2 = Bidirectional(GRU( 50, return_sequences=False, name=\"Sentence_GRU2\"))\n",
    "# tgru1 = Bidirectional(GRU( 50, return_sequences=True, name=\"Triplet_GRU1\"))\n",
    "# tgru2 = Bidirectional(GRU( 50, return_sequences=False, name=\"Triplet_GRU2\"))\n",
    "\n",
    "gru1 =  (GRU( gru_dimension,  return_sequences=True,  name=\"Sentence_GRU1\"))\n",
    "gru2 =  (GRU( gru_dimension,  return_sequences=True,  name=\"Sentence_GRU2\"))\n",
    "tgru1 = (GRU( gru_dimension,  return_sequences=True,  name=\"Triplet_GRU1\"))\n",
    "tgru2 = (GRU( gru_dimension,  return_sequences=True, name=\"Triplet_GRU2\"))\n",
    "\n",
    "mean_layer = Lambda(lambda xin: mean(xin, axis=1), name=\"SentenceMean\")\n",
    "\n",
    "repeat_layer       = RepeatVector(triplets_input_shape[0], name=\"AttentionRepeat\")\n",
    "sum_layer          = Lambda(lambda xin: sum(xin, axis=-1), name=\"AttentionDotsum\")\n",
    "soft_max_layer     = Softmax(name=\"AttentionSofMax\")\n",
    "weighted_sum_layer = Lambda(lambda X: sum(X[0]*expand_dims(X[1],axis=-1), axis=1), name=\"AttentionWeightedSum\")\n",
    "\n",
    "X_question = gru2(gru1(question_vectors_input))             # (None, 3000, 50) -> (None, 60, 50)\n",
    "X_question = Multiply()([question_masks_input, X_question]) # (None, 60, 50)\n",
    "X_question = mean_layer(X_question)                         # (None, 60, 50) -> (None, 50)\n",
    "\n",
    "X_triplets = Dense(gru_dimension, activation='relu', name=\"Dense50T\")(triplets_input)    # (None,500,900) -> (None,500,50)\n",
    "X_triplets = tgru2(tgru1(X_triplets))                                         # (None,500,50)  -> (None,500,50)\n",
    "\n",
    "X_attention = repeat_layer(X_question)\n",
    "# compute Wa*X_triplets\n",
    "X_triplets_wegithed = Dense(gru_dimension, use_bias=False, name=\"AttentionDense\")(X_triplets)\n",
    "# compute dot of (Wa*X_triplets) with X_question\n",
    "X_attention = Multiply(name=\"AttentionDotMul\")([X_triplets_wegithed, X_attention])\n",
    "X_attention = sum_layer(X_attention)\n",
    "# compute softmax and get attention weights\n",
    "X_attention = soft_max_layer(X_attention)\n",
    "# weighted sum of triplets with attention weights\n",
    "X_triplets = weighted_sum_layer([ X_triplets, X_attention])\n",
    "\n",
    "X_concatenated = Concatenate(name=\"SentenceTripletConcat\")([X_question, X_triplets])                      # (None,100)\n",
    "X_concatenated = Dense(64, activation='relu', name=\"Dense64\")(X_concatenated) # (None,100) -> (None,64)        \n",
    "Y_pred = Dense(1, activation='sigmoid', name=\"Dense1\")(X_concatenated)        # (None,64)  -> sigmoid    \n",
    "\n",
    "model = Model(inputs=[question_vectors_input, question_masks_input, triplets_input], outputs=Y_pred)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.fit([question_vectors, question_masks, triplet_vectors], Y, epochs = 30, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "#TRAIN\n",
    "s_NumEpocs        = 5\n",
    "s_NumFilesInBatch = 5\n",
    "file_list         = glob.glob('X:/cs230/proj0/ned-graphs/data_wiki_encoded/train/train_*.ipwk')\n",
    "file_list_count   = len(file_list)\n",
    "file_batch_count  = math.ceil(( file_list_count / s_NumFilesInBatch ))\n",
    "\n",
    "print(file_list_count)\n",
    "print(file_batch_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Epoch  0 batch  5  Files [5] start  0 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 19s 19ms/step - loss: 0.6891 - acc: 0.5490\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6722 - acc: 0.6510\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6436 - acc: 0.7290\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.6050 - acc: 0.7610\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.5549 - acc: 0.7870\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.5099 - acc: 0.7970\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4749 - acc: 0.8160\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4355 - acc: 0.8120\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4064 - acc: 0.8320\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3850 - acc: 0.8410\n",
      "Training on Epoch  0 batch  5  Files [5] start  5 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 24s 24ms/step - loss: 0.5001 - acc: 0.7950\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 25s 25ms/step - loss: 0.4560 - acc: 0.8270\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 26s 26ms/step - loss: 0.4235 - acc: 0.8390\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 26s 26ms/step - loss: 0.3992 - acc: 0.8450\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 27s 27ms/step - loss: 0.3507 - acc: 0.8630\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 27s 27ms/step - loss: 0.3239 - acc: 0.8710\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 26s 26ms/step - loss: 0.3056 - acc: 0.8760\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 25s 25ms/step - loss: 0.2725 - acc: 0.8930\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 27s 27ms/step - loss: 0.2549 - acc: 0.9050\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 29s 29ms/step - loss: 0.2432 - acc: 0.9150\n",
      "Training on Epoch  0 batch  5  Files [5] start  10 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 23s 23ms/step - loss: 0.6386 - acc: 0.7980\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 29s 29ms/step - loss: 0.4855 - acc: 0.8160\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.4360 - acc: 0.8220\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 36s 36ms/step - loss: 0.4061 - acc: 0.8390\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.3836 - acc: 0.8420\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3534 - acc: 0.8630\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.3561 - acc: 0.8660\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 30s 30ms/step - loss: 0.3461 - acc: 0.8750\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 29s 29ms/step - loss: 0.3361 - acc: 0.8730\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 28s 28ms/step - loss: 0.3102 - acc: 0.8820\n",
      "Training on Epoch  0 batch  5  Files [5] start  15 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.4703 - acc: 0.8240\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.4045 - acc: 0.8280\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 31s 31ms/step - loss: 0.3783 - acc: 0.8380\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 31s 31ms/step - loss: 0.3549 - acc: 0.8580\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.3299 - acc: 0.8620\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.3141 - acc: 0.8760\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 31s 31ms/step - loss: 0.2944 - acc: 0.8770\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 30s 30ms/step - loss: 0.2769 - acc: 0.8850\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 30s 30ms/step - loss: 0.2664 - acc: 0.9000\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.2572 - acc: 0.8990\n",
      "Training on Epoch  0 batch  5  Files [5] start  20 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 28s 28ms/step - loss: 0.4468 - acc: 0.8220\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 30s 30ms/step - loss: 0.3909 - acc: 0.8390\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.3709 - acc: 0.8470\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.3495 - acc: 0.8540\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.3278 - acc: 0.8680\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.3079 - acc: 0.8690\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2928 - acc: 0.8770\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2731 - acc: 0.8870\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.2524 - acc: 0.8990\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.2430 - acc: 0.8980\n",
      "Training on Epoch  0 batch  5  Files [5] start  25 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 30s 30ms/step - loss: 0.4676 - acc: 0.8200\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.3998 - acc: 0.8370\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 36s 36ms/step - loss: 0.3881 - acc: 0.8390\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3706 - acc: 0.8510\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 0.3577 - acc: 0.8560\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3468 - acc: 0.8500\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.3333 - acc: 0.8620\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.3179 - acc: 0.8630\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.3044 - acc: 0.8740\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2923 - acc: 0.8780\n",
      "Training on Epoch  0 batch  5  Files [5] start  30 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.4257 - acc: 0.8250\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3860 - acc: 0.8350\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.3691 - acc: 0.8480\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.3445 - acc: 0.8550\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 47s 47ms/step - loss: 0.3232 - acc: 0.8730\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.3088 - acc: 0.8700\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2874 - acc: 0.8790\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2714 - acc: 0.8880\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.2560 - acc: 0.8930\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.2401 - acc: 0.9000\n",
      "Training on Epoch  0 batch  5  Files [5] start  35 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3841 - acc: 0.8440\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 47s 47ms/step - loss: 0.3482 - acc: 0.8450\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.3239 - acc: 0.8540\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.2967 - acc: 0.8710\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.2779 - acc: 0.8760\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 47s 47ms/step - loss: 0.2622 - acc: 0.8780\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2462 - acc: 0.9000\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.2239 - acc: 0.9060\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 47s 47ms/step - loss: 0.2099 - acc: 0.9060\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2001 - acc: 0.9110\n",
      "Training on Epoch  0 batch  5  Files [5] start  40 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.4066 - acc: 0.8570\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.3530 - acc: 0.8580\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.3136 - acc: 0.8720\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.3009 - acc: 0.8750\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 72s 72ms/step - loss: 0.2730 - acc: 0.8910\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 82s 82ms/step - loss: 0.2589 - acc: 0.9010\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 67s 67ms/step - loss: 0.2445 - acc: 0.8990\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 47s 47ms/step - loss: 0.2323 - acc: 0.9050\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.2241 - acc: 0.9120\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2061 - acc: 0.9200\n",
      "Training on Epoch  0 batch  5  Files [5] start  45 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.4164 - acc: 0.8310\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.3547 - acc: 0.8540\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 47s 47ms/step - loss: 0.3343 - acc: 0.8620\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.3169 - acc: 0.8650\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.3044 - acc: 0.8740\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.2877 - acc: 0.8900\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.2718 - acc: 0.8930\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2585 - acc: 0.9030\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2452 - acc: 0.9050\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2345 - acc: 0.9200\n",
      "Training on Epoch  0 batch  5  Files [5] start  50 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.4640 - acc: 0.7970\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.5040 - acc: 0.7440\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.4570 - acc: 0.7790\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.4318 - acc: 0.8000\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.3865 - acc: 0.8230\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3509 - acc: 0.8390\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3477 - acc: 0.8470\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.3453 - acc: 0.8570\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3454 - acc: 0.8440\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3276 - acc: 0.8600\n",
      "Training on Epoch  0 batch  5  Files [5] start  55 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.4456 - acc: 0.8190\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.4165 - acc: 0.8310\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3915 - acc: 0.8340\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.3697 - acc: 0.8350\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.3457 - acc: 0.8540\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.3344 - acc: 0.8710\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.3182 - acc: 0.8740\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.3068 - acc: 0.8730\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2913 - acc: 0.8820\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.2781 - acc: 0.8840\n",
      "Training on Epoch  0 batch  5  Files [5] start  60 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.4121 - acc: 0.8170\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3815 - acc: 0.8320\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.3495 - acc: 0.8460\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.3326 - acc: 0.8610\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.3148 - acc: 0.8670\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.2979 - acc: 0.8720\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.2774 - acc: 0.8840\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.2645 - acc: 0.8940\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 47s 47ms/step - loss: 0.2557 - acc: 0.8970\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.2433 - acc: 0.9040\n",
      "Training on Epoch  0 batch  5  Files [5] start  65 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3670 - acc: 0.8520\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3167 - acc: 0.8720\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2929 - acc: 0.8890\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2799 - acc: 0.8970\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2655 - acc: 0.8940\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2522 - acc: 0.8980\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2382 - acc: 0.9100\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2250 - acc: 0.9160\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2114 - acc: 0.9230\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.1983 - acc: 0.9300\n",
      "Training on Epoch  0 batch  5  Files [5] start  70 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3960 - acc: 0.8450\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.3335 - acc: 0.8660\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.2965 - acc: 0.8870\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2755 - acc: 0.8970\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.2913 - acc: 0.8890\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2679 - acc: 0.8940\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.2482 - acc: 0.9060\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2321 - acc: 0.9150\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2108 - acc: 0.9180\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.2008 - acc: 0.9160\n",
      "Training on Epoch  0 batch  5  Files [5] start  75 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.4300 - acc: 0.8140\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.3830 - acc: 0.8210\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.3852 - acc: 0.8180\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3569 - acc: 0.8430\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.3257 - acc: 0.8460\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3046 - acc: 0.8640\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2860 - acc: 0.8750\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2696 - acc: 0.8830\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2523 - acc: 0.8880\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2324 - acc: 0.8970\n",
      "Training on Epoch  0 batch  5  Files [5] start  80 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.4751 - acc: 0.8010\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.4152 - acc: 0.8170\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3775 - acc: 0.8320\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.3512 - acc: 0.8660\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3291 - acc: 0.8600\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3092 - acc: 0.8720\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.2918 - acc: 0.8800\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.2766 - acc: 0.8880\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.2604 - acc: 0.8990\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2498 - acc: 0.9010\n",
      "Training on Epoch  0 batch  5  Files [5] start  85 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.4268 - acc: 0.8330\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.3778 - acc: 0.8510\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.3477 - acc: 0.8600\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.3204 - acc: 0.8750\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2995 - acc: 0.8790\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2791 - acc: 0.8930\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2609 - acc: 0.9050\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2467 - acc: 0.9030\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2341 - acc: 0.9110\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2240 - acc: 0.9200\n",
      "Training on Epoch  0 batch  5  Files [5] start  90 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.4040 - acc: 0.8380\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3592 - acc: 0.8540\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3351 - acc: 0.8720\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3042 - acc: 0.8820\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2799 - acc: 0.8950\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2622 - acc: 0.9030\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2451 - acc: 0.9180\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2286 - acc: 0.9230\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2153 - acc: 0.9310\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2009 - acc: 0.9350\n",
      "Training on Epoch  0 batch  5  Files [5] start  95 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 41s 41ms/step - loss: 0.4201 - acc: 0.8290\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 41s 41ms/step - loss: 0.3624 - acc: 0.8560\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 41s 41ms/step - loss: 0.3285 - acc: 0.8670\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 41s 41ms/step - loss: 0.3012 - acc: 0.8800\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2745 - acc: 0.8930\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2622 - acc: 0.8930\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2398 - acc: 0.9030\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2315 - acc: 0.9080\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2171 - acc: 0.9210\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2183 - acc: 0.9260\n",
      "Training on Epoch  0 batch  5  Files [5] start  100 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 41s 41ms/step - loss: 0.3755 - acc: 0.8540\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.3033 - acc: 0.8760\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2782 - acc: 0.8860\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2545 - acc: 0.9010\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2324 - acc: 0.9090\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.2114 - acc: 0.9180\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.1922 - acc: 0.9330\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.1826 - acc: 0.9320\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.1685 - acc: 0.9380\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.1556 - acc: 0.9440\n",
      "Training on Epoch  0 batch  5  Files [5] start  105 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.3620 - acc: 0.8480\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.3230 - acc: 0.8670\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 45s 45ms/step - loss: 0.2738 - acc: 0.8920\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.2764 - acc: 0.8940\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 47s 47ms/step - loss: 0.2597 - acc: 0.9000\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2347 - acc: 0.9140\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2182 - acc: 0.9190\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.1951 - acc: 0.9320\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 47s 47ms/step - loss: 0.1817 - acc: 0.9310\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.1665 - acc: 0.9420\n",
      "Training on Epoch  0 batch  5  Files [5] start  110 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.4637 - acc: 0.8190\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.3848 - acc: 0.8450\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.3328 - acc: 0.8560\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.3198 - acc: 0.8650\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.3001 - acc: 0.8750\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2723 - acc: 0.8950\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2508 - acc: 0.8960\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2348 - acc: 0.9080\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2157 - acc: 0.9220\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2005 - acc: 0.9230\n",
      "Training on Epoch  0 batch  5  Files [5] start  115 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.4221 - acc: 0.8190\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.3587 - acc: 0.8370\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.3368 - acc: 0.8540\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.3139 - acc: 0.8620\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2923 - acc: 0.8850\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2778 - acc: 0.8880\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2634 - acc: 0.8910\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2773 - acc: 0.8850\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2643 - acc: 0.8870\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2468 - acc: 0.9000\n",
      "Training on Epoch  0 batch  5  Files [5] start  120 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 47s 47ms/step - loss: 0.3193 - acc: 0.8580\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.3020 - acc: 0.8660\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2847 - acc: 0.8860\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2800 - acc: 0.8870\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2766 - acc: 0.8900\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2683 - acc: 0.8760\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2491 - acc: 0.8940\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2452 - acc: 0.8900\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2351 - acc: 0.8970\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2243 - acc: 0.9030\n",
      "Training on Epoch  0 batch  5  Files [5] start  125 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.4355 - acc: 0.8230\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.3490 - acc: 0.8450\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.3212 - acc: 0.8540\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.3031 - acc: 0.8640\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2918 - acc: 0.8750\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.2731 - acc: 0.8780\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.2625 - acc: 0.8890\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2476 - acc: 0.8950\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2436 - acc: 0.8890\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2318 - acc: 0.8980\n",
      "Training on Epoch  0 batch  5  Files [5] start  130 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.3908 - acc: 0.8280\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.3600 - acc: 0.8470\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.3332 - acc: 0.8580\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.3140 - acc: 0.8640\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2903 - acc: 0.8780\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2730 - acc: 0.8820\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2679 - acc: 0.8840\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2570 - acc: 0.8900\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2395 - acc: 0.8970\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2279 - acc: 0.9060\n",
      "Training on Epoch  0 batch  5  Files [5] start  135 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.3946 - acc: 0.8300\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.3419 - acc: 0.8320\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.3142 - acc: 0.8530\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.3018 - acc: 0.8580\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2896 - acc: 0.8760\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2832 - acc: 0.8720\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2754 - acc: 0.8820\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2670 - acc: 0.8750\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.2569 - acc: 0.8880\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2606 - acc: 0.8820\n",
      "Training on Epoch  0 batch  5  Files [5] start  140 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.3703 - acc: 0.8370\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.3322 - acc: 0.8400\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2964 - acc: 0.8640\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2755 - acc: 0.8810\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2512 - acc: 0.8970\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2385 - acc: 0.8970\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2226 - acc: 0.9110\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.2081 - acc: 0.9150\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1983 - acc: 0.9110\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1881 - acc: 0.9190\n",
      "Training on Epoch  0 batch  5  Files [5] start  145 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.3488 - acc: 0.8730\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.2889 - acc: 0.8890\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2434 - acc: 0.9080\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2239 - acc: 0.9170\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2102 - acc: 0.9200\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.1812 - acc: 0.9410\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.1662 - acc: 0.9470\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.1503 - acc: 0.9490\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.1321 - acc: 0.9510\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.1151 - acc: 0.9560\n",
      "Training on Epoch  0 batch  5  Files [5] start  150 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.4214 - acc: 0.8540\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.3377 - acc: 0.8650\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.2771 - acc: 0.8920\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.2487 - acc: 0.9100\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.2257 - acc: 0.9200\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.2148 - acc: 0.9240\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2054 - acc: 0.9290\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.1852 - acc: 0.9360\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1749 - acc: 0.9440\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1653 - acc: 0.9400\n",
      "Training on Epoch  0 batch  5  Files [5] start  155 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2872 - acc: 0.8790\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2373 - acc: 0.9030\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2076 - acc: 0.9080\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1850 - acc: 0.9270\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1791 - acc: 0.9330\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1695 - acc: 0.9420\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1812 - acc: 0.9330\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1772 - acc: 0.9270\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1668 - acc: 0.9330\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1480 - acc: 0.9370\n",
      "Training on Epoch  0 batch  5  Files [5] start  160 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.3074 - acc: 0.8810\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.2443 - acc: 0.8980\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2155 - acc: 0.9130\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.1978 - acc: 0.9190\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.1828 - acc: 0.9280\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.1696 - acc: 0.9340\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.1555 - acc: 0.9410\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.1435 - acc: 0.9440\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.1315 - acc: 0.9470\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.1225 - acc: 0.9510\n",
      "Training on Epoch  0 batch  5  Files [5] start  165 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.2755 - acc: 0.8680\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.2231 - acc: 0.9010\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.2004 - acc: 0.8980\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1797 - acc: 0.9090\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1688 - acc: 0.9220\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1559 - acc: 0.9290\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1461 - acc: 0.9310\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1389 - acc: 0.9350\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1291 - acc: 0.9430\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1227 - acc: 0.9440\n",
      "Training on Epoch  0 batch  5  Files [5] start  170 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.2956 - acc: 0.9070\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.2609 - acc: 0.9060\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.2106 - acc: 0.9210\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1830 - acc: 0.9260\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1647 - acc: 0.9340\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1536 - acc: 0.9450\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1457 - acc: 0.9410\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1317 - acc: 0.9410\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 6613s 7s/step - loss: 0.1256 - acc: 0.9520\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1188 - acc: 0.9590\n",
      "Training on Epoch  0 batch  5  Files [5] start  175 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.2471 - acc: 0.9010\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1959 - acc: 0.9200\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1620 - acc: 0.9340\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1481 - acc: 0.9380\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1357 - acc: 0.9510\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1235 - acc: 0.9530\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1135 - acc: 0.9580\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1044 - acc: 0.9580\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.0973 - acc: 0.9640\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.0914 - acc: 0.9620\n",
      "Training on Epoch  0 batch  5  Files [5] start  180 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1581 - acc: 0.9320\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1504 - acc: 0.9230\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1170 - acc: 0.9410\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1073 - acc: 0.9520\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1015 - acc: 0.9560\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.0935 - acc: 0.9580\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.0911 - acc: 0.9590\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0819 - acc: 0.9650\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.0845 - acc: 0.9620\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.0768 - acc: 0.9660\n",
      "Training on Epoch  0 batch  5  Files [5] start  185 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.2311 - acc: 0.9000\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.1788 - acc: 0.9220\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1604 - acc: 0.9290\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.1555 - acc: 0.9280\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1603 - acc: 0.9190\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1415 - acc: 0.9350\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.1387 - acc: 0.9340\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 0.1335 - acc: 0.9350\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1242 - acc: 0.9400\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.1223 - acc: 0.9370\n",
      "Training on Epoch  0 batch  5  Files [5] start  190 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.2312 - acc: 0.9020\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.1916 - acc: 0.9140\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.1718 - acc: 0.9210\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.1517 - acc: 0.9320\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.1419 - acc: 0.9280\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.1334 - acc: 0.9410\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.1258 - acc: 0.9420\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.1187 - acc: 0.9490\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 49s 49ms/step - loss: 0.1148 - acc: 0.9420\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.1144 - acc: 0.9450\n",
      "Training on Epoch  0 batch  5  Files [5] start  195 / 459\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.2811 - acc: 0.8720\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.2493 - acc: 0.8830\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.2260 - acc: 0.8700\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.2089 - acc: 0.8950\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1968 - acc: 0.8880\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.1869 - acc: 0.9040\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.1770 - acc: 0.9070\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.1718 - acc: 0.9140\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.1649 - acc: 0.9130\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1569 - acc: 0.9180\n",
      "Training on Epoch  0 batch  5  Files [5] start  200 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.2591 - acc: 0.8830\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2211 - acc: 0.8990\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1894 - acc: 0.9060\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1674 - acc: 0.9100\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1575 - acc: 0.9180\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1475 - acc: 0.9240\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1372 - acc: 0.9350\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1289 - acc: 0.9350\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1225 - acc: 0.9400\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1100 - acc: 0.9430\n",
      "Training on Epoch  0 batch  5  Files [5] start  205 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1898 - acc: 0.8970\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1619 - acc: 0.9150\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1443 - acc: 0.9250\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1361 - acc: 0.9180\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1274 - acc: 0.9300\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1230 - acc: 0.9320\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1183 - acc: 0.9290\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1167 - acc: 0.9350\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1181 - acc: 0.9300\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1118 - acc: 0.9370\n",
      "Training on Epoch  0 batch  5  Files [5] start  210 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.2275 - acc: 0.9080\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1584 - acc: 0.9370\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1276 - acc: 0.9580\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1123 - acc: 0.9620\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.0979 - acc: 0.9640\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.0858 - acc: 0.9710\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.0820 - acc: 0.9690\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.0730 - acc: 0.9720\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.0697 - acc: 0.9760\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.0635 - acc: 0.9760\n",
      "Training on Epoch  0 batch  5  Files [5] start  215 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.3050 - acc: 0.8770\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.2208 - acc: 0.9010\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.2118 - acc: 0.8950\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1917 - acc: 0.9100\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1806 - acc: 0.9140\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1736 - acc: 0.9150\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1646 - acc: 0.9230\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1537 - acc: 0.9270\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1489 - acc: 0.9240\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1447 - acc: 0.9300\n",
      "Training on Epoch  0 batch  5  Files [5] start  220 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.3736 - acc: 0.8620\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.2722 - acc: 0.8890\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.2313 - acc: 0.9090\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.2112 - acc: 0.9110\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1981 - acc: 0.9150\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1847 - acc: 0.9140\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1691 - acc: 0.9310\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1588 - acc: 0.9270\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1490 - acc: 0.9320\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1387 - acc: 0.9420\n",
      "Training on Epoch  0 batch  5  Files [5] start  225 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.2165 - acc: 0.9230\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1782 - acc: 0.9270\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1539 - acc: 0.9390\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1419 - acc: 0.9360\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1270 - acc: 0.9500\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1174 - acc: 0.9560\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1069 - acc: 0.9580\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0979 - acc: 0.9610\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0882 - acc: 0.9680\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0901 - acc: 0.9630\n",
      "Training on Epoch  0 batch  5  Files [5] start  230 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1403 - acc: 0.9360\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1189 - acc: 0.9420\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1078 - acc: 0.9440\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1015 - acc: 0.9500\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.0956 - acc: 0.9540\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.0929 - acc: 0.9520\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.0888 - acc: 0.9610\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.0863 - acc: 0.9600\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.0869 - acc: 0.9640\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.0824 - acc: 0.9660\n",
      "Training on Epoch  0 batch  5  Files [5] start  235 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.2495 - acc: 0.9000\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.3163 - acc: 0.8730\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.2134 - acc: 0.8940\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.1981 - acc: 0.9030\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.1575 - acc: 0.9290\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.1639 - acc: 0.9190\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 0.1443 - acc: 0.9350\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.1441 - acc: 0.9290\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.1366 - acc: 0.9360\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.1297 - acc: 0.9420\n",
      "Training on Epoch  0 batch  5  Files [5] start  240 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.1370 - acc: 0.9370\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.1257 - acc: 0.9470\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.1181 - acc: 0.9500\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.1092 - acc: 0.9510\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.1029 - acc: 0.9550\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.0957 - acc: 0.9640\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.0922 - acc: 0.9640\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.0881 - acc: 0.9640\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 0.0858 - acc: 0.9640\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 51s 51ms/step - loss: 0.0826 - acc: 0.9660\n",
      "Training on Epoch  0 batch  5  Files [5] start  245 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1863 - acc: 0.9120\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1718 - acc: 0.9200\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1592 - acc: 0.9230\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1480 - acc: 0.9270\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1390 - acc: 0.9310\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1316 - acc: 0.9330\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 0.1256 - acc: 0.9360\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1179 - acc: 0.9430\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1131 - acc: 0.9440\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1079 - acc: 0.9450\n",
      "Training on Epoch  0 batch  5  Files [5] start  250 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.2469 - acc: 0.8840\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.2056 - acc: 0.8840\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1745 - acc: 0.8990\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1606 - acc: 0.9090\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1512 - acc: 0.9160\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1439 - acc: 0.9240\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1393 - acc: 0.9300\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1341 - acc: 0.9310\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1318 - acc: 0.9310\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1249 - acc: 0.9360\n",
      "Training on Epoch  0 batch  5  Files [5] start  255 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1636 - acc: 0.9190\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1463 - acc: 0.9290\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1323 - acc: 0.9410\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1231 - acc: 0.9440\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1207 - acc: 0.9460\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1171 - acc: 0.9400\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1094 - acc: 0.9510\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1093 - acc: 0.9490\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1017 - acc: 0.9520\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0973 - acc: 0.9540\n",
      "Training on Epoch  0 batch  5  Files [5] start  260 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.2397 - acc: 0.8950\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.2050 - acc: 0.9040\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.1838 - acc: 0.9060\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1701 - acc: 0.9160\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1589 - acc: 0.9200\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1512 - acc: 0.9270\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1405 - acc: 0.9280\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1336 - acc: 0.9360\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1298 - acc: 0.9340\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1228 - acc: 0.9440\n",
      "Training on Epoch  0 batch  5  Files [5] start  265 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.2405 - acc: 0.8920\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1996 - acc: 0.9110\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1704 - acc: 0.9260\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1540 - acc: 0.9310\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1476 - acc: 0.9370\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1390 - acc: 0.9410\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1289 - acc: 0.9450\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1182 - acc: 0.9510\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1125 - acc: 0.9580\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1057 - acc: 0.9540\n",
      "Training on Epoch  0 batch  5  Files [5] start  270 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 64s 64ms/step - loss: 0.2077 - acc: 0.9020\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 64s 64ms/step - loss: 0.1828 - acc: 0.9090\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 64s 64ms/step - loss: 0.1624 - acc: 0.9210\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 64s 64ms/step - loss: 0.1487 - acc: 0.9330\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 64s 64ms/step - loss: 0.1381 - acc: 0.9420\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 67s 67ms/step - loss: 0.1341 - acc: 0.9430\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 65s 65ms/step - loss: 0.1230 - acc: 0.9430\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 65s 65ms/step - loss: 0.1173 - acc: 0.9510\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 65s 65ms/step - loss: 0.1171 - acc: 0.9470\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 66s 66ms/step - loss: 0.1079 - acc: 0.9540\n",
      "Training on Epoch  0 batch  5  Files [5] start  275 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.2321 - acc: 0.9140\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1823 - acc: 0.9280\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1475 - acc: 0.9430\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1265 - acc: 0.9520\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.1180 - acc: 0.9550\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 0.1054 - acc: 0.9530\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0968 - acc: 0.9590\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.0895 - acc: 0.9590\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.0825 - acc: 0.9650\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 0.0764 - acc: 0.9690\n",
      "Training on Epoch  0 batch  5  Files [5] start  280 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1882 - acc: 0.9120\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1539 - acc: 0.9270\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1369 - acc: 0.9360\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.1249 - acc: 0.9470\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1164 - acc: 0.9510\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 0.1083 - acc: 0.9540\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 0.1012 - acc: 0.9550\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.0949 - acc: 0.9560\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.0895 - acc: 0.9560\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 54s 54ms/step - loss: 0.0886 - acc: 0.9590\n",
      "Training on Epoch  0 batch  5  Files [5] start  285 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.2213 - acc: 0.8870\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.1822 - acc: 0.9030\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.1635 - acc: 0.9180\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1519 - acc: 0.9270\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1428 - acc: 0.9350\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1378 - acc: 0.9310\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 0.1302 - acc: 0.9360\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1282 - acc: 0.9390\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.1245 - acc: 0.9380\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.1205 - acc: 0.9410\n",
      "Training on Epoch  0 batch  5  Files [5] start  290 / 459\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 0.2413 - acc: 0.8970\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 0.2015 - acc: 0.9090\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 0.1743 - acc: 0.9210\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.1572 - acc: 0.9350\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.1515 - acc: 0.9340\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.1416 - acc: 0.9370\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 0.1303 - acc: 0.9430\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 0.1242 - acc: 0.9430\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 0.1189 - acc: 0.9480\n",
      "Epoch 10/10\n",
      " 800/1000 [=======================>......] - ETA: 12s - loss: 0.1013 - acc: 0.9525"
     ]
    }
   ],
   "source": [
    "for ep in range(0,s_NumEpocs):\n",
    "    if ep != 0:\n",
    "        file_name = \"0\n",
    "        \" + str(ep-1) + \".save\"\n",
    "        model.load_weights(file_name)\n",
    "    \n",
    "    file_index       = 0\n",
    "    batch_file_index = 0\n",
    "    \n",
    "    data = []\n",
    "    # train on batches of files\n",
    "    for file_index in range(0, file_list_count) :\n",
    "        inputFile = file_list[file_index]\n",
    "        file_index       += 1\n",
    "        batch_file_index += 1\n",
    "        # print(inputFile)\n",
    "        f = open(inputFile, 'rb')\n",
    "        test = pickle.load(f)\n",
    "        data = np.append(data, test)\n",
    "        f.close()\n",
    "    \n",
    "        if batch_file_index == s_NumFilesInBatch or file_index == file_list_count:\n",
    "            print(\"Training on Epoch \", str(ep), \"batch \", str(batch_file_index), \" Files [\" + str(s_NumFilesInBatch) + \"] start \", str(file_index - batch_file_index), \"/\", str(file_list_count) )\n",
    "            question_vectors, question_masks, triplet_vectors, Y = prepare_data(data, \n",
    "                                                                                question_max_words   = s_QuestionMaxWords, \n",
    "                                                                                triplets_max_numbers = s_TripletsMaxNumbers, \n",
    "                                                                                dimOfQuestionVector  = s_DimOfQuestionVector, \n",
    "                                                                                dimOfTripletVector   = s_DimOfTripletVector, \n",
    "                                                                                dimOfMask            = s_DimOfMask )\n",
    "            # train on this batch\n",
    "            model.fit([question_vectors, question_masks, triplet_vectors], Y, epochs = 10, batch_size = 200, shuffle=True)\n",
    "            # rest for next batch\n",
    "            batch_file_index = 0;\n",
    "            data = []\n",
    "            \n",
    "    # save the mode for the epoch\n",
    "    file_name = \"train_epoch_general_attention_\" + str(ep) + \".save\"\n",
    "    model.save_weights(file_name)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Input, Dropout, LSTM, Activation, SimpleRNN, GRU, Concatenate, Multiply, Reshape, Flatten, Bidirectional\n",
    "\n",
    "# # layers\n",
    "# s1_bilstm = Bidirectional(GRU(128, return_sequences = True), name = \"s1_bilstm\")\n",
    "# s2_bilstm = Bidirectional(GRU(128, return_sequences = False), name = \"s2_bilstm\")\n",
    "# t_bilstm = Bidirectional(GRU(128, return_sequences = False), name = \"t_bilstm\")\n",
    "# embedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\n",
    "# unstack_layer = Lambda(lambda xin: myUnstack(xin), name=\"AUnstackLayer\")\n",
    "\n",
    "# # sentence inputs\n",
    "# sentence_indices = Input(shape=(maxWordsPerSentence,), dtype='int32', name=\"sentence_indices\")   \n",
    "# sentence_masks = Input(shape=(2*lstm_dim,), dtype='float32', name=\"sentence_masks\")\n",
    "\n",
    "# # sentence embeddings\n",
    "# sentence_embeddings = embedding_layer(sentence_indices)       \n",
    "# # X_sentence = Reshape((maxWordsPerSentence, -1), name=\"s_reshape\")(sentence_embeddings)\n",
    "# X_sentence = s1_bilstm(sentence_embeddings)\n",
    "# X_sentence = Multiply()([sentence_masks, X_sentence]) # (None, 60, 256)\n",
    "# X_sentence = s2_bilstm(X_sentence)\n",
    "\n",
    "# # triplets\n",
    "# triplets_input = Input(shape=(maxTriplets, maxWordsPerSentence,), dtype='int32', name=\"tripletz\") \n",
    "# triplet_embeddings = embedding_layer(triplets_input)\n",
    "\n",
    "# X_triplets = Reshape((maxTriplets, -1))(triplet_embeddings)\n",
    "# X_triplets = t_bilstm(X_triplets)\n",
    "\n",
    "# X_concatenated = Concatenate()([X_sentence, X_triplets])\n",
    "# X_concatenated = Dense(256, activation='relu', name=\"Dense256\")(X_concatenated)\n",
    "# X_concatenated = Dense(64, activation='relu', name=\"Dense64\")(X_concatenated)\n",
    "# Y_pred = Dense(1, activation='sigmoid', name=\"Dense1\")(X_concatenated)\n",
    "\n",
    "# m.v2 = Model(inputs=[sentence_indices, sentence_masks, triplets_input], outputs=X_concatenated)\n",
    "# print(m.v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
