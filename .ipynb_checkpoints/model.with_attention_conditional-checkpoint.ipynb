{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import import_ipynb\n",
    "#from model import *\n",
    "import numpy as np\n",
    "import itertools\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.backend import mean, sum, expand_dims, tanh\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, GRU, Concatenate, Multiply, Reshape, RepeatVector, Dot, Softmax\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Lambda\n",
    "from keras.optimizers import Adam\n",
    "np.random.seed(1)\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given x (?,dim) always return (max_len, dim) by padding with zeros\n",
    "def padding(max_len, x, dim ):\n",
    "    x_len = x.shape[0]\n",
    "    # truncate if more than max_len else pad with zeros\n",
    "    if(x_len >= max_len):\n",
    "        return x[0:max_len]\n",
    "    else:   \n",
    "        padding = np.zeros((max_len-x_len, dim))     \n",
    "        if(len(x.shape) > 1):\n",
    "            assert dim == x.shape[1],\"dimensions didnt match in padding\"   \n",
    "            return np.concatenate((x, padding))\n",
    "        else:\n",
    "            return padding\n",
    "        \n",
    "# given 1D array x, we return array of shape (max_len, dim) by replicating each element dim times and apdding with zeros at end\n",
    "def reshape_mask(max_len, x, dim):\n",
    "    # mask is a 1D array so padd with dim == 1\n",
    "    padded_question_mask = np.squeeze(padding(max_len, x, 1))\n",
    "    final_question_mask = []\n",
    "    for qm in padded_question_mask:\n",
    "        # make each mask index into array of dimension dim\n",
    "        final_question_mask.append(np.array([qm]*dim))\n",
    "    return np.asarray(final_question_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s_DimOfQuestionVector = 300\n",
    "s_DimOfTripletVector  = 900\n",
    "s_DimOfMask           = 50\n",
    "s_QuestionMaxWords    = 60\n",
    "s_TripletsMaxNumbers  = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:/cs230/proj0/ned-graphs/data_wiki_encoded/train/train_8.ipwk\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(data, \n",
    "                 question_max_words, \n",
    "                 triplets_max_numbers, \n",
    "                 dimOfQuestionVector, \n",
    "                 dimOfTripletVector, \n",
    "                 dimOfMask):\n",
    "    question_vectors = []\n",
    "    question_masks = []\n",
    "    triplet_vectors = []\n",
    "    Y = []\n",
    "    for elem in data:\n",
    "        item_question             = elem[\"question\"]\n",
    "        padded_item_question      = padding(question_max_words, item_question, dimOfQuestionVector)\n",
    "        item_question_mask        = np.reshape(elem[\"question_mask\"], (len(elem[\"question_mask\"]), 1) )\n",
    "        padded_item_question_mask = reshape_mask(question_max_words, item_question_mask, dimOfMask)\n",
    "        item_correct_triplets     = elem[\"correct_triplets\"]\n",
    "        item_wrong_triplets       = elem[\"wrong_triplets\"]\n",
    "        \n",
    "        # for triplets take mean of each S, P  and 0 to get 3,300 vectors which are then concatenated to dimOfTripletVector vector\n",
    "        item_correct_triplets_vectors = [] \n",
    "        for tr in item_correct_triplets: # There are triplets_max_numbers triplets\n",
    "            # Each tr has s, p and o. Each of the _s, _p and _o's are (num_tokens, dimOfTripletVector) size\n",
    "            _s = np.mean(tr[0], axis=0)\n",
    "            _p = np.mean(tr[1], axis=0)\n",
    "            _o = np.mean(tr[2], axis=0)\n",
    "            item_correct_triplets_vectors.append(np.concatenate((_s, _p, _o))) # adding (900,) vector for each triplet\n",
    "        #pad triplet vectors so we have (triplets_max_numbers, dimOfTripletVector)  encoding for all triplets\n",
    "        padded_item_triplets_vectors = padding(triplets_max_numbers, np.asarray(item_correct_triplets_vectors), dimOfTripletVector)\n",
    "        \n",
    "        triplet_vectors.append(padded_item_triplets_vectors)\n",
    "        question_vectors.append(padded_item_question)\n",
    "        question_masks.append(padded_item_question_mask)\n",
    "        Y.append(1)\n",
    "\n",
    "        # for wrong triplets\n",
    "        item_wrong_triplets_vectors = []\n",
    "        for tr in item_wrong_triplets:\n",
    "            _s = np.mean(tr[0], axis=0)\n",
    "            _p = np.mean(tr[1], axis=0)\n",
    "            _o = np.mean(tr[2], axis=0)\n",
    "            item_wrong_triplets_vectors.append(np.concatenate((_s, _p, _o)))\n",
    "        padded_item_wrong_triplets_vectors = padding(triplets_max_numbers, np.asarray(item_wrong_triplets_vectors), dimOfTripletVector)\n",
    "                \n",
    "        triplet_vectors.append(padded_item_wrong_triplets_vectors)\n",
    "        question_vectors.append(padded_item_question)\n",
    "        question_masks.append(padded_item_question_mask)\n",
    "        Y.append(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    question_vectors = np.asarray(question_vectors)\n",
    "    question_masks   = np.asarray(question_masks)\n",
    "    triplet_vectors  = np.asarray(triplet_vectors)\n",
    "    Y                = np.asarray(Y)\n",
    "    return question_vectors, question_masks, triplet_vectors, Y\n",
    "\n",
    "def generate_model_inputs():\n",
    "    file_list = glob.glob('X:/cs230/proj0/ned-graphs/data_wiki_encoded/train/train_8.ipwk')\n",
    "    data = []\n",
    "    num = 0\n",
    "    for inputFile in file_list:\n",
    "        print(inputFile)\n",
    "        f = open(inputFile, 'rb')\n",
    "        test = pickle.load(f)\n",
    "        data = np.append(data, test)\n",
    "        f.close()\n",
    "        num += 1\n",
    "        if num == 1:\n",
    "            break\n",
    "    \n",
    "    return prepare_data(data, \n",
    "                        question_max_words   = s_QuestionMaxWords, \n",
    "                        triplets_max_numbers = s_TripletsMaxNumbers, \n",
    "                        dimOfQuestionVector  = s_DimOfQuestionVector, \n",
    "                        dimOfTripletVector   = s_DimOfTripletVector, \n",
    "                        dimOfMask            = s_DimOfMask )\n",
    "\n",
    "question_vectors, question_masks, triplet_vectors, Y = generate_model_inputs()#prepare_data(data, question_max_words=60, triplets_max_numbers=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 60, 300)\n",
      "(200, 60, 50)\n",
      "(200, 500, 900)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(question_vectors.shape)\n",
    "print(question_masks.shape)\n",
    "print(triplet_vectors.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_input_shape      = (s_QuestionMaxWords,   s_DimOfQuestionVector)\n",
    "questions_mask_input_shape = (s_QuestionMaxWords,   s_DimOfMask )\n",
    "triplets_input_shape       = (s_TripletsMaxNumbers, s_DimOfTripletVector )\n",
    "# question_vectors(, 60, 300), question_masks(, 60, 300), triplet_vectors (,500,900), Y\n",
    "\n",
    "gru_dimension = s_DimOfMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "question_vectors_input (InputLa (None, 60, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Sentence_GRU1 (GRU)             (None, 60, 50)       52650       question_vectors_input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "question_masks_input (InputLaye (None, 60, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Sentence_GRU2 (GRU)             (None, 60, 50)       15150       Sentence_GRU1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 60, 50)       0           question_masks_input[0][0]       \n",
      "                                                                 Sentence_GRU2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "triplets_input (InputLayer)     (None, 500, 900)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SentenceMean (Lambda)           (None, 50)           0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dense50T (Dense)                (None, 500, 50)      45050       triplets_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Triplet_GRU1 (GRU)              (None, 500, 50)      15150       Dense50T[0][0]                   \n",
      "                                                                 SentenceMean[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Triplet_GRU2 (GRU)              (None, 500, 50)      15150       Triplet_GRU1[0][0]               \n",
      "                                                                 SentenceMean[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "AttentionDense (Dense)          (None, 500, 50)      2500        Triplet_GRU2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "AttentionRepeat (RepeatVector)  (None, 500, 50)      0           SentenceMean[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "AttentionDotMul (Multiply)      (None, 500, 50)      0           AttentionDense[0][0]             \n",
      "                                                                 AttentionRepeat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "AttentionDotsum (Lambda)        (None, 500)          0           AttentionDotMul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "AttentionSofMax (Softmax)       (None, 500)          0           AttentionDotsum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "AttentionWeightedSum (Lambda)   (None, 50)           0           Triplet_GRU2[0][0]               \n",
      "                                                                 AttentionSofMax[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SentenceTripletConcat (Concaten (None, 100)          0           SentenceMean[0][0]               \n",
      "                                                                 AttentionWeightedSum[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Dense64 (Dense)                 (None, 64)           6464        SentenceTripletConcat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Dense1 (Dense)                  (None, 1)            65          Dense64[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 152,179\n",
      "Trainable params: 152,179\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# question inputs\n",
    "question_vectors_input = Input(shape=questions_input_shape,      dtype='float32', name=\"question_vectors_input\")   \n",
    "question_masks_input   = Input(shape=questions_mask_input_shape, dtype='float32', name=\"question_masks_input\")\n",
    "\n",
    "# triplets input\n",
    "triplets_input = Input(shape=triplets_input_shape, dtype='float32', name=\"triplets_input\")  #(None, 500, 900)\n",
    "\n",
    "# layers\n",
    "# gru1 = Bidirectional(GRU( 50, return_sequences=True, name=\"Sentence_GRU1\"))\n",
    "# gru2 = Bidirectional(GRU( 50, return_sequences=False, name=\"Sentence_GRU2\"))\n",
    "# tgru1 = Bidirectional(GRU( 50, return_sequences=True, name=\"Triplet_GRU1\"))\n",
    "# tgru2 = Bidirectional(GRU( 50, return_sequences=False, name=\"Triplet_GRU2\"))\n",
    "\n",
    "gru1 =  (GRU( gru_dimension,  return_sequences=True,  name=\"Sentence_GRU1\"))\n",
    "gru2 =  (GRU( gru_dimension,  return_sequences=True,  name=\"Sentence_GRU2\"))\n",
    "tgru1 = (GRU( gru_dimension,  return_sequences=True,  name=\"Triplet_GRU1\"))\n",
    "tgru2 = (GRU( gru_dimension,  return_sequences=True, name=\"Triplet_GRU2\"))\n",
    "\n",
    "mean_layer = Lambda(lambda xin: mean(xin, axis=1), name=\"SentenceMean\")\n",
    "\n",
    "repeat_layer       = RepeatVector(triplets_input_shape[0], name=\"AttentionRepeat\")\n",
    "sum_layer          = Lambda(lambda xin: sum(xin, axis=-1), name=\"AttentionDotsum\")\n",
    "soft_max_layer     = Softmax(name=\"AttentionSofMax\")\n",
    "weighted_sum_layer = Lambda(lambda X: sum(X[0]*expand_dims(X[1],axis=-1), axis=1), name=\"AttentionWeightedSum\")\n",
    "\n",
    "X_question = gru2(gru1(question_vectors_input))             # (None, 3000, 50) -> (None, 60, 50)\n",
    "X_question = Multiply()([question_masks_input, X_question]) # (None, 60, 50)\n",
    "X_question = mean_layer(X_question)                         # (None, 60, 50) -> (None, 50)\n",
    "\n",
    "X_triplets = Dense(gru_dimension, activation='relu', name=\"Dense50T\")(triplets_input)      # (None,500,900) -> (None,500,50)\n",
    "X_triplets = tgru2(tgru1(X_triplets, initial_state=X_question), initial_state=X_question)  # (None,500,50)  -> (None,500,50)\n",
    "\n",
    "X_attention = repeat_layer(X_question)\n",
    "# compute Wa*X_triplets\n",
    "X_triplets_wegithed = Dense(gru_dimension, use_bias=False, name=\"AttentionDense\")(X_triplets)\n",
    "# compute dot of (Wa*X_triplets) with X_question\n",
    "X_attention = Multiply(name=\"AttentionDotMul\")([X_triplets_wegithed, X_attention])\n",
    "X_attention = sum_layer(X_attention)\n",
    "# compute softmax and get attention weights\n",
    "X_attention = soft_max_layer(X_attention)\n",
    "# weighted sum of triplets with attention weights\n",
    "X_triplets = weighted_sum_layer([ X_triplets, X_attention])\n",
    "\n",
    "X_concatenated = Concatenate(name=\"SentenceTripletConcat\")([X_question, X_triplets])                      # (None,100)\n",
    "X_concatenated = Dense(64, activation='relu', name=\"Dense64\")(X_concatenated) # (None,100) -> (None,64)        \n",
    "Y_pred = Dense(1, activation='sigmoid', name=\"Dense1\")(X_concatenated)        # (None,64)  -> sigmoid    \n",
    "\n",
    "model = Model(inputs=[question_vectors_input, question_masks_input, triplets_input], outputs=Y_pred)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.6974 - acc: 0.4950\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 6s 30ms/step - loss: 0.6787 - acc: 0.5150\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 6s 31ms/step - loss: 0.6686 - acc: 0.5750\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 6s 31ms/step - loss: 0.6359 - acc: 0.6700\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 6s 29ms/step - loss: 0.5889 - acc: 0.6900\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.5321 - acc: 0.8100\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 6s 30ms/step - loss: 0.4357 - acc: 0.8700\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.4508 - acc: 0.8650\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 6s 29ms/step - loss: 0.3866 - acc: 0.8600\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 6s 29ms/step - loss: 0.3471 - acc: 0.8650\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 6s 29ms/step - loss: 0.2940 - acc: 0.9000\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 6s 29ms/step - loss: 0.2532 - acc: 0.9100\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.2335 - acc: 0.9250\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 6s 29ms/step - loss: 0.2004 - acc: 0.9300\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 6s 29ms/step - loss: 0.1991 - acc: 0.9300\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 6s 31ms/step - loss: 0.2043 - acc: 0.9200\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 6s 29ms/step - loss: 0.1973 - acc: 0.9100\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1390 - acc: 0.9400\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 8s 38ms/step - loss: 0.2044 - acc: 0.9150\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 0.1879 - acc: 0.9250\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.1749 - acc: 0.9300\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 8s 38ms/step - loss: 0.1601 - acc: 0.9350\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.1570 - acc: 0.9450\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 6s 31ms/step - loss: 0.1285 - acc: 0.9400\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.1179 - acc: 0.9400\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.1264 - acc: 0.9400\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1176 - acc: 0.9500\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 6s 29ms/step - loss: 0.1080 - acc: 0.9550\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.1102 - acc: 0.9400\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1052 - acc: 0.9350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26aaaae9320>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit([question_vectors, question_masks, triplet_vectors], Y, epochs = 30, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def EvalModel( model ):\n",
    "    file_list         = glob.glob('X:/cs230/proj0/ned-graphs/data_wiki_encoded/dev/dev_*.ipwk')\n",
    "    file_list_count   = 2 #len(file_list)\n",
    "    \n",
    "    data = []\n",
    "    file_index       = 0\n",
    "    # train on batches of files\n",
    "    for file_index in range(0, file_list_count) :\n",
    "        inputFile = file_list[file_index]\n",
    "        file_index       += 1\n",
    "        # print(inputFile)\n",
    "        f = open(inputFile, 'rb')\n",
    "        test = pickle.load(f)\n",
    "        data = np.append(data, test)\n",
    "        f.close()\n",
    "    \n",
    "    question_vectors, question_masks, triplet_vectors, Y = prepare_data(data, \n",
    "                                                                                question_max_words   = s_QuestionMaxWords, \n",
    "                                                                                triplets_max_numbers = s_TripletsMaxNumbers, \n",
    "                                                                                dimOfQuestionVector  = s_DimOfQuestionVector, \n",
    "                                                                                dimOfTripletVector   = s_DimOfTripletVector, \n",
    "                                                                                dimOfMask            = s_DimOfMask )\n",
    "        \n",
    "    \n",
    "    loss, acc = model.evaluate([question_vectors, question_masks, triplet_vectors], Y)\n",
    "    print( \"loss = \" + str(loss) + \" acc  \" + str(acc))\n",
    "    Y_pred = model.predict([question_vectors, question_masks, triplet_vectors])\n",
    "    Y_pred = np.squeeze(Y_pred)\n",
    "    #print(y.astype(np.float))\n",
    "    print(confusion_matrix(Y, Y_pred>0.5))\n",
    "    print(classification_report(Y, Y_pred>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 6s 14ms/step\n",
      "loss = 1.1573289012908936 acc  0.735\n",
      "[[158  42]\n",
      " [ 64 136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.75       200\n",
      "           1       0.76      0.68      0.72       200\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       400\n",
      "   macro avg       0.74      0.74      0.73       400\n",
      "weighted avg       0.74      0.73      0.73       400\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-9191e6d67b3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEvalModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "(y, y_pred) = EvalModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.75       200\n",
      "           1       0.76      0.68      0.72       200\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       400\n",
      "   macro avg       0.74      0.74      0.73       400\n",
      "weighted avg       0.74      0.73      0.73       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.squeeze(y_pred)\n",
    "#print(y.astype(np.float))\n",
    "confusion_matrix(y, y_pred>0.5)\n",
    "print(classification_report(y, y_pred>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "s_NumEpocs        = 5\n",
    "s_NumFilesInBatch = 5\n",
    "file_list         = glob.glob('X:/cs230/proj0/ned-graphs/data_wiki_encoded/train/train_*.ipwk')\n",
    "file_list_count   = len(file_list)\n",
    "file_batch_count  = math.ceil(( file_list_count / s_NumFilesInBatch ))\n",
    "\n",
    "print(file_list_count)\n",
    "print(file_batch_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ep in range(0,s_NumEpocs):\n",
    "    if ep != 0:\n",
    "        file_name = \"train_epoch_general_attention_\" + str(ep-1) + \".save\"\n",
    "        model.load_weights(file_name)\n",
    "    \n",
    "    file_index       = 0\n",
    "    batch_file_index = 0\n",
    "    \n",
    "    data = []\n",
    "    # train on batches of files\n",
    "    for file_index in range(0, file_list_count) :\n",
    "        inputFile = file_list[file_index]\n",
    "        file_index       += 1\n",
    "        batch_file_index += 1\n",
    "        # print(inputFile)\n",
    "        f = open(inputFile, 'rb')\n",
    "        test = pickle.load(f)\n",
    "        data = np.append(data, test)\n",
    "        f.close()\n",
    "    \n",
    "        if batch_file_index == s_NumFilesInBatch or file_index == file_list_count:\n",
    "            print(\"Training on Epoch \", str(ep), \"batch \", str(batch_file_index), \" Files [\" + str(s_NumFilesInBatch) + \"] start \", str(file_index - batch_file_index), \"/\", str(file_list_count) )\n",
    "            question_vectors, question_masks, triplet_vectors, Y = prepare_data(data, \n",
    "                                                                                question_max_words   = s_QuestionMaxWords, \n",
    "                                                                                triplets_max_numbers = s_TripletsMaxNumbers, \n",
    "                                                                                dimOfQuestionVector  = s_DimOfQuestionVector, \n",
    "                                                                                dimOfTripletVector   = s_DimOfTripletVector, \n",
    "                                                                                dimOfMask            = s_DimOfMask )\n",
    "            # train on this batch\n",
    "            model.fit([question_vectors, question_masks, triplet_vectors], Y, epochs = 10, batch_size = 200, shuffle=True)\n",
    "            # rest for next batch\n",
    "            batch_file_index = 0;\n",
    "            data = []\n",
    "            \n",
    "    # save the mode for the epoch\n",
    "    file_name = \"train_epoch_general_attention_\" + str(ep) + \".save\"\n",
    "    model.save_weights(file_name)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Input, Dropout, LSTM, Activation, SimpleRNN, GRU, Concatenate, Multiply, Reshape, Flatten, Bidirectional\n",
    "\n",
    "# # layers\n",
    "# s1_bilstm = Bidirectional(GRU(128, return_sequences = True), name = \"s1_bilstm\")\n",
    "# s2_bilstm = Bidirectional(GRU(128, return_sequences = False), name = \"s2_bilstm\")\n",
    "# t_bilstm = Bidirectional(GRU(128, return_sequences = False), name = \"t_bilstm\")\n",
    "# embedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\n",
    "# unstack_layer = Lambda(lambda xin: myUnstack(xin), name=\"AUnstackLayer\")\n",
    "\n",
    "# # sentence inputs\n",
    "# sentence_indices = Input(shape=(maxWordsPerSentence,), dtype='int32', name=\"sentence_indices\")   \n",
    "# sentence_masks = Input(shape=(2*lstm_dim,), dtype='float32', name=\"sentence_masks\")\n",
    "\n",
    "# # sentence embeddings\n",
    "# sentence_embeddings = embedding_layer(sentence_indices)       \n",
    "# # X_sentence = Reshape((maxWordsPerSentence, -1), name=\"s_reshape\")(sentence_embeddings)\n",
    "# X_sentence = s1_bilstm(sentence_embeddings)\n",
    "# X_sentence = Multiply()([sentence_masks, X_sentence]) # (None, 60, 256)\n",
    "# X_sentence = s2_bilstm(X_sentence)\n",
    "\n",
    "# # triplets\n",
    "# triplets_input = Input(shape=(maxTriplets, maxWordsPerSentence,), dtype='int32', name=\"tripletz\") \n",
    "# triplet_embeddings = embedding_layer(triplets_input)\n",
    "\n",
    "# X_triplets = Reshape((maxTriplets, -1))(triplet_embeddings)\n",
    "# X_triplets = t_bilstm(X_triplets)\n",
    "\n",
    "# X_concatenated = Concatenate()([X_sentence, X_triplets])\n",
    "# X_concatenated = Dense(256, activation='relu', name=\"Dense256\")(X_concatenated)\n",
    "# X_concatenated = Dense(64, activation='relu', name=\"Dense64\")(X_concatenated)\n",
    "# Y_pred = Dense(1, activation='sigmoid', name=\"Dense1\")(X_concatenated)\n",
    "\n",
    "# m.v2 = Model(inputs=[sentence_indices, sentence_masks, triplets_input], outputs=X_concatenated)\n",
    "# print(m.v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
