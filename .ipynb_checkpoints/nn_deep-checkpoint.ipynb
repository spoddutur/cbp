{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(x):\n",
    "#     \"\"\"\n",
    "#     Compute the sigmoid of x\n",
    "#     Arguments:\n",
    "#     x -- A scalar or numpy array of any size.\n",
    "#     Return:\n",
    "#     s -- sigmoid(x)\n",
    "#     \"\"\"\n",
    "#     s = 1/(1+np.exp(-x))\n",
    "#     return s\n",
    "\n",
    "# def relu(x):\n",
    "#     \"\"\"\n",
    "#     Compute the relu of x\n",
    "#     Arguments:\n",
    "#     x -- A scalar or numpy array of any size.\n",
    "#     Return:\n",
    "#     s -- relu(x)\n",
    "#     \"\"\"\n",
    "#     s = np.maximum(0,x)\n",
    "    \n",
    "#     return s\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = (4, 5)\n",
      "b1 = (4, 1)\n",
      "W2 = (3, 4)\n",
      "b2 = (3, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"].shape))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"].shape))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"].shape))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A)+b\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    elif activation == \"tanh\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A = np.tanh(Z)\n",
    "        activation_cache = Z\n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"tanh\")\n",
    "        caches.append(cache)\n",
    "      \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = -1/m*np.sum(np.multiply(np.log(AL),Y)+np.multiply(np.log(1-AL),(1-Y)))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m*np.dot(dZ,A_prev.T)\n",
    "    db = 1/m*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "      \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l + 1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "  \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/surthi/Documents/mygitrepo/cs230-code-examples/tensorflow/nlp/cs230/lib/python3.6/site-packages/pandas/core/frame.py:3391: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "articleLengthN                         float64\n",
       "countOfDistinctIndustryN               float64\n",
       "countOfOccurenceN                      float64\n",
       "countOfOccurenceOfAliasesN             float64\n",
       "countOfOccurenceOfNamePreserveListN    float64\n",
       "countOfSignificantStringsN             float64\n",
       "countOfUrlMatchN                       float64\n",
       "countryMatchByOverlapScore             float64\n",
       "directCityMatchScore                   float64\n",
       "directCountryMatchScore                float64\n",
       "directStateMatchScore                  float64\n",
       "exactMatch                             float64\n",
       "fieldMatch                             float64\n",
       "isCommon                               float64\n",
       "isIndustry                             float64\n",
       "isProminent                            float64\n",
       "isProminentByItself                    float64\n",
       "isProminentByParent                    float64\n",
       "isSingleCompany                        float64\n",
       "isTickerMatch                          float64\n",
       "isUrlMatched                           float64\n",
       "personScore                            float64\n",
       "stateMatchByOverlapScore               float64\n",
       "sumOfIndustryN                         float64\n",
       "w2vScoreN                              float64\n",
       "isGold                                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "contents = open(\"/Users/surthi/Downloads/Archive (1)/mixed_features_gold_tagged_cleared_betterN/part-00000-e97dbd07-b5c8-4160-9c31-6430e44a03ac.json\", \"r\").read() \n",
    "json_data = [json.loads(str(item)) for item in contents.strip().split('\\n')]\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "all_features_df = json_normalize(json_data)\n",
    "#print(all_features_df.columns.values)\n",
    "\n",
    "df = all_features_df[['articleLengthN', 'countOfDistinctIndustryN', 'countOfOccurenceN', 'countOfOccurenceOfAliasesN', \n",
    "      'countOfOccurenceOfNamePreserveListN', 'countOfSignificantStringsN', 'countOfUrlMatchN', \n",
    "      'countryMatchByOverlapScore', 'directCityMatchScore', 'directCountryMatchScore', 'directStateMatchScore',\n",
    "     'exactMatch', 'fieldMatch', 'isCommon', 'isIndustry', 'isProminent',\n",
    " 'isProminentByItself', 'isProminentByParent',  'isSingleCompany', 'isTickerMatch', \n",
    "      'isUrlMatched', 'personScore', 'stateMatchByOverlapScore', 'sumOfIndustryN', 'w2vScoreN','isGold' ]]\n",
    "\n",
    "# convert boolean features to 0's and 1's:\n",
    "df[['isCommon', 'isIndustry', 'isProminent', 'fieldMatch',\n",
    " 'isProminentByItself', 'isProminentByParent',  'isSingleCompany', 'isTickerMatch', \n",
    "      'isUrlMatched', 'isGold','exactMatch']] *= 1.0\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "X_test = test.loc[:, test.columns != 'isGold'].T.values\n",
    "Y_test = test.loc[:, test.columns == 'isGold'].T.values\n",
    "X_train = train.loc[:, test.columns != 'isGold'].T.values\n",
    "Y_train = train.loc[:, test.columns == 'isGold'].T.values\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693183\n",
      "Cost after iteration 100: 0.685990\n",
      "Cost after iteration 200: 0.679151\n",
      "Cost after iteration 300: 0.672646\n",
      "Cost after iteration 400: 0.666447\n",
      "Cost after iteration 500: 0.660557\n",
      "Cost after iteration 600: 0.654956\n",
      "Cost after iteration 700: 0.649628\n",
      "Cost after iteration 800: 0.644559\n",
      "Cost after iteration 900: 0.639735\n",
      "Cost after iteration 1000: 0.635144\n",
      "Cost after iteration 1100: 0.630773\n",
      "Cost after iteration 1200: 0.626610\n",
      "Cost after iteration 1300: 0.622643\n",
      "Cost after iteration 1400: 0.618869\n",
      "Cost after iteration 1500: 0.615274\n",
      "Cost after iteration 1600: 0.611850\n",
      "Cost after iteration 1700: 0.608588\n",
      "Cost after iteration 1800: 0.605480\n",
      "Cost after iteration 1900: 0.602518\n",
      "Cost after iteration 2000: 0.599695\n",
      "Cost after iteration 2100: 0.597004\n",
      "Cost after iteration 2200: 0.594437\n",
      "Cost after iteration 2300: 0.591990\n",
      "Cost after iteration 2400: 0.589656\n",
      "Cost after iteration 2500: 0.587429\n",
      "Cost after iteration 2600: 0.585304\n",
      "Cost after iteration 2700: 0.583276\n",
      "Cost after iteration 2800: 0.581341\n",
      "Cost after iteration 2900: 0.579493\n",
      "Cost after iteration 3000: 0.577729\n",
      "Cost after iteration 3100: 0.576044\n",
      "Cost after iteration 3200: 0.574435\n",
      "Cost after iteration 3300: 0.572897\n",
      "Cost after iteration 3400: 0.571429\n",
      "Cost after iteration 3500: 0.570025\n",
      "Cost after iteration 3600: 0.568683\n",
      "Cost after iteration 3700: 0.567401\n",
      "Cost after iteration 3800: 0.566175\n",
      "Cost after iteration 3900: 0.565003\n",
      "Cost after iteration 4000: 0.563881\n",
      "Cost after iteration 4100: 0.562809\n",
      "Cost after iteration 4200: 0.561783\n",
      "Cost after iteration 4300: 0.560801\n",
      "Cost after iteration 4400: 0.559861\n",
      "Cost after iteration 4500: 0.558962\n",
      "Cost after iteration 4600: 0.558101\n",
      "Cost after iteration 4700: 0.557277\n",
      "Cost after iteration 4800: 0.556487\n",
      "Cost after iteration 4900: 0.555731\n",
      "Cost after iteration 5000: 0.555007\n",
      "Cost after iteration 5100: 0.554314\n",
      "Cost after iteration 5200: 0.553649\n",
      "Cost after iteration 5300: 0.553012\n",
      "Cost after iteration 5400: 0.552402\n",
      "Cost after iteration 5500: 0.551817\n",
      "Cost after iteration 5600: 0.551257\n",
      "Cost after iteration 5700: 0.550719\n",
      "Cost after iteration 5800: 0.550204\n",
      "Cost after iteration 5900: 0.549709\n",
      "Cost after iteration 6000: 0.549235\n",
      "Cost after iteration 6100: 0.548781\n",
      "Cost after iteration 6200: 0.548345\n",
      "Cost after iteration 6300: 0.547926\n",
      "Cost after iteration 6400: 0.547525\n",
      "Cost after iteration 6500: 0.547139\n",
      "Cost after iteration 6600: 0.546770\n",
      "Cost after iteration 6700: 0.546415\n",
      "Cost after iteration 6800: 0.546074\n",
      "Cost after iteration 6900: 0.545747\n",
      "Cost after iteration 7000: 0.545434\n",
      "Cost after iteration 7100: 0.545132\n",
      "Cost after iteration 7200: 0.544843\n",
      "Cost after iteration 7300: 0.544566\n",
      "Cost after iteration 7400: 0.544298\n",
      "Cost after iteration 7500: 0.544041\n",
      "Cost after iteration 7600: 0.543794\n",
      "Cost after iteration 7700: 0.543557\n",
      "Cost after iteration 7800: 0.543329\n",
      "Cost after iteration 7900: 0.543110\n",
      "Cost after iteration 8000: 0.542899\n",
      "Cost after iteration 8100: 0.542696\n",
      "Cost after iteration 8200: 0.542501\n",
      "Cost after iteration 8300: 0.542314\n",
      "Cost after iteration 8400: 0.542134\n",
      "Cost after iteration 8500: 0.541961\n",
      "Cost after iteration 8600: 0.541790\n",
      "Cost after iteration 8700: 0.541628\n",
      "Cost after iteration 8800: 0.541472\n",
      "Cost after iteration 8900: 0.541323\n",
      "Cost after iteration 9000: 0.541180\n",
      "Cost after iteration 9100: 0.541042\n",
      "Cost after iteration 9200: 0.540910\n",
      "Cost after iteration 9300: 0.540783\n",
      "Cost after iteration 9400: 0.540661\n",
      "Cost after iteration 9500: 0.540543\n",
      "Cost after iteration 9600: 0.540429\n",
      "Cost after iteration 9700: 0.540320\n",
      "Cost after iteration 9800: 0.540215\n",
      "Cost after iteration 9900: 0.540113\n",
      "Cost after iteration 10000: 0.540016\n",
      "Cost after iteration 10100: 0.539922\n",
      "Cost after iteration 10200: 0.539832\n",
      "Cost after iteration 10300: 0.539745\n",
      "Cost after iteration 10400: 0.539661\n",
      "Cost after iteration 10500: 0.539580\n",
      "Cost after iteration 10600: 0.539503\n",
      "Cost after iteration 10700: 0.539428\n",
      "Cost after iteration 10800: 0.539356\n",
      "Cost after iteration 10900: 0.539285\n",
      "Cost after iteration 11000: 0.539217\n",
      "Cost after iteration 11100: 0.539151\n",
      "Cost after iteration 11200: 0.539087\n",
      "Cost after iteration 11300: 0.539025\n",
      "Cost after iteration 11400: 0.538966\n",
      "Cost after iteration 11500: 0.538909\n",
      "Cost after iteration 11600: 0.538853\n",
      "Cost after iteration 11700: 0.538799\n",
      "Cost after iteration 11800: 0.538747\n",
      "Cost after iteration 11900: 0.538697\n",
      "Cost after iteration 12000: 0.538648\n",
      "Cost after iteration 12100: 0.538600\n",
      "Cost after iteration 12200: 0.538554\n",
      "Cost after iteration 12300: 0.538510\n",
      "Cost after iteration 12400: 0.538467\n",
      "Cost after iteration 12500: 0.538425\n",
      "Cost after iteration 12600: 0.538384\n",
      "Cost after iteration 12700: 0.538345\n",
      "Cost after iteration 12800: 0.538307\n",
      "Cost after iteration 12900: 0.538269\n",
      "Cost after iteration 13000: 0.538233\n",
      "Cost after iteration 13100: 0.538197\n",
      "Cost after iteration 13200: 0.538163\n",
      "Cost after iteration 13300: 0.538129\n",
      "Cost after iteration 13400: 0.538096\n",
      "Cost after iteration 13500: 0.538064\n",
      "Cost after iteration 13600: 0.538033\n",
      "Cost after iteration 13700: 0.538003\n",
      "Cost after iteration 13800: 0.537973\n",
      "Cost after iteration 13900: 0.537944\n",
      "Cost after iteration 14000: 0.537916\n",
      "Cost after iteration 14100: 0.537888\n",
      "Cost after iteration 14200: 0.537861\n",
      "Cost after iteration 14300: 0.537834\n",
      "Cost after iteration 14400: 0.537808\n",
      "Cost after iteration 14500: 0.537782\n",
      "Cost after iteration 14600: 0.537757\n",
      "Cost after iteration 14700: 0.537732\n",
      "Cost after iteration 14800: 0.537708\n",
      "Cost after iteration 14900: 0.537683\n",
      "Cost after iteration 15000: 0.537660\n",
      "Cost after iteration 15100: 0.537637\n",
      "Cost after iteration 15200: 0.537614\n",
      "Cost after iteration 15300: 0.537590\n",
      "Cost after iteration 15400: 0.537567\n",
      "Cost after iteration 15500: 0.537544\n",
      "Cost after iteration 15600: 0.537522\n",
      "Cost after iteration 15700: 0.537499\n",
      "Cost after iteration 15800: 0.537476\n",
      "Cost after iteration 15900: 0.537454\n",
      "Cost after iteration 16000: 0.537432\n",
      "Cost after iteration 16100: 0.537409\n",
      "Cost after iteration 16200: 0.537387\n",
      "Cost after iteration 16300: 0.537364\n",
      "Cost after iteration 16400: 0.537342\n",
      "Cost after iteration 16500: 0.537319\n",
      "Cost after iteration 16600: 0.537297\n",
      "Cost after iteration 16700: 0.537274\n",
      "Cost after iteration 16800: 0.537251\n",
      "Cost after iteration 16900: 0.537227\n",
      "Cost after iteration 17000: 0.537204\n",
      "Cost after iteration 17100: 0.537180\n",
      "Cost after iteration 17200: 0.537156\n",
      "Cost after iteration 17300: 0.537132\n",
      "Cost after iteration 17400: 0.537107\n",
      "Cost after iteration 17500: 0.537082\n",
      "Cost after iteration 17600: 0.537057\n",
      "Cost after iteration 17700: 0.537031\n",
      "Cost after iteration 17800: 0.537005\n",
      "Cost after iteration 17900: 0.536978\n",
      "Cost after iteration 18000: 0.536950\n",
      "Cost after iteration 18100: 0.536922\n",
      "Cost after iteration 18200: 0.536894\n",
      "Cost after iteration 18300: 0.536865\n",
      "Cost after iteration 18400: 0.536835\n",
      "Cost after iteration 18500: 0.536805\n",
      "Cost after iteration 18600: 0.536774\n",
      "Cost after iteration 18700: 0.536742\n",
      "Cost after iteration 18800: 0.536709\n",
      "Cost after iteration 18900: 0.536677\n",
      "Cost after iteration 19000: 0.536644\n",
      "Cost after iteration 19100: 0.536612\n",
      "Cost after iteration 19200: 0.536578\n",
      "Cost after iteration 19300: 0.536544\n",
      "Cost after iteration 19400: 0.536509\n",
      "Cost after iteration 19500: 0.536473\n",
      "Cost after iteration 19600: 0.536436\n",
      "Cost after iteration 19700: 0.536399\n",
      "Cost after iteration 19800: 0.536360\n",
      "Cost after iteration 19900: 0.536320\n",
      "Cost after iteration 20000: 0.536279\n",
      "Cost after iteration 20100: 0.536237\n",
      "Cost after iteration 20200: 0.536194\n",
      "Cost after iteration 20300: 0.536150\n",
      "Cost after iteration 20400: 0.536105\n",
      "Cost after iteration 20500: 0.536058\n",
      "Cost after iteration 20600: 0.536011\n",
      "Cost after iteration 20700: 0.535963\n",
      "Cost after iteration 20800: 0.535913\n",
      "Cost after iteration 20900: 0.535862\n",
      "Cost after iteration 21000: 0.535810\n",
      "Cost after iteration 21100: 0.535756\n",
      "Cost after iteration 21200: 0.535700\n",
      "Cost after iteration 21300: 0.535643\n",
      "Cost after iteration 21400: 0.535585\n",
      "Cost after iteration 21500: 0.535525\n",
      "Cost after iteration 21600: 0.535463\n",
      "Cost after iteration 21700: 0.535399\n",
      "Cost after iteration 21800: 0.535334\n",
      "Cost after iteration 21900: 0.535267\n",
      "Cost after iteration 22000: 0.535198\n",
      "Cost after iteration 22100: 0.535127\n",
      "Cost after iteration 22200: 0.535055\n",
      "Cost after iteration 22300: 0.534980\n",
      "Cost after iteration 22400: 0.534904\n",
      "Cost after iteration 22500: 0.534825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 22600: 0.534744\n",
      "Cost after iteration 22700: 0.534661\n",
      "Cost after iteration 22800: 0.534576\n",
      "Cost after iteration 22900: 0.534488\n",
      "Cost after iteration 23000: 0.534398\n",
      "Cost after iteration 23100: 0.534305\n",
      "Cost after iteration 23200: 0.534209\n",
      "Cost after iteration 23300: 0.534112\n",
      "Cost after iteration 23400: 0.534011\n",
      "Cost after iteration 23500: 0.533908\n",
      "Cost after iteration 23600: 0.533803\n",
      "Cost after iteration 23700: 0.533695\n",
      "Cost after iteration 23800: 0.533584\n",
      "Cost after iteration 23900: 0.533470\n",
      "Cost after iteration 24000: 0.533353\n",
      "Cost after iteration 24100: 0.533233\n",
      "Cost after iteration 24200: 0.533109\n",
      "Cost after iteration 24300: 0.532981\n",
      "Cost after iteration 24400: 0.532851\n",
      "Cost after iteration 24500: 0.532717\n",
      "Cost after iteration 24600: 0.532579\n",
      "Cost after iteration 24700: 0.532438\n",
      "Cost after iteration 24800: 0.532292\n",
      "Cost after iteration 24900: 0.532143\n",
      "Cost after iteration 25000: 0.531992\n",
      "Cost after iteration 25100: 0.531836\n",
      "Cost after iteration 25200: 0.531677\n",
      "Cost after iteration 25300: 0.531513\n",
      "Cost after iteration 25400: 0.531345\n",
      "Cost after iteration 25500: 0.531173\n",
      "Cost after iteration 25600: 0.531000\n",
      "Cost after iteration 25700: 0.530821\n",
      "Cost after iteration 25800: 0.530635\n",
      "Cost after iteration 25900: 0.530444\n",
      "Cost after iteration 26000: 0.530249\n",
      "Cost after iteration 26100: 0.530048\n",
      "Cost after iteration 26200: 0.529843\n",
      "Cost after iteration 26300: 0.529632\n",
      "Cost after iteration 26400: 0.529418\n",
      "Cost after iteration 26500: 0.529197\n",
      "Cost after iteration 26600: 0.528969\n",
      "Cost after iteration 26700: 0.528738\n",
      "Cost after iteration 26800: 0.528498\n",
      "Cost after iteration 26900: 0.528258\n",
      "Cost after iteration 27000: 0.528012\n",
      "Cost after iteration 27100: 0.527755\n",
      "Cost after iteration 27200: 0.527460\n",
      "Cost after iteration 27300: 0.527174\n",
      "Cost after iteration 27400: 0.526903\n",
      "Cost after iteration 27500: 0.526631\n",
      "Cost after iteration 27600: 0.526348\n",
      "Cost after iteration 27700: 0.526055\n",
      "Cost after iteration 27800: 0.525769\n",
      "Cost after iteration 27900: 0.525468\n",
      "Cost after iteration 28000: 0.525161\n",
      "Cost after iteration 28100: 0.524845\n",
      "Cost after iteration 28200: 0.524528\n",
      "Cost after iteration 28300: 0.524195\n",
      "Cost after iteration 28400: 0.523857\n",
      "Cost after iteration 28500: 0.523508\n",
      "Cost after iteration 28600: 0.523152\n",
      "Cost after iteration 28700: 0.522785\n",
      "Cost after iteration 28800: 0.522418\n",
      "Cost after iteration 28900: 0.522039\n",
      "Cost after iteration 29000: 0.521637\n",
      "Cost after iteration 29100: 0.521181\n",
      "Cost after iteration 29200: 0.520788\n",
      "Cost after iteration 29300: 0.520378\n",
      "Cost after iteration 29400: 0.519966\n",
      "Cost after iteration 29500: 0.519546\n",
      "Cost after iteration 29600: 0.519116\n",
      "Cost after iteration 29700: 0.518674\n",
      "Cost after iteration 29800: 0.518240\n",
      "Cost after iteration 29900: 0.517778\n",
      "Cost after iteration 30000: 0.517332\n",
      "Cost after iteration 30100: 0.516868\n",
      "Cost after iteration 30200: 0.516401\n",
      "Cost after iteration 30300: 0.515926\n",
      "Cost after iteration 30400: 0.515441\n",
      "Cost after iteration 30500: 0.514951\n",
      "Cost after iteration 30600: 0.514438\n",
      "Cost after iteration 30700: 0.513845\n",
      "Cost after iteration 30800: 0.513320\n",
      "Cost after iteration 30900: 0.512803\n",
      "Cost after iteration 31000: 0.512280\n",
      "Cost after iteration 31100: 0.511746\n",
      "Cost after iteration 31200: 0.511209\n",
      "Cost after iteration 31300: 0.510663\n",
      "Cost after iteration 31400: 0.510084\n",
      "Cost after iteration 31500: 0.509432\n",
      "Cost after iteration 31600: 0.508886\n",
      "Cost after iteration 31700: 0.508291\n",
      "Cost after iteration 31800: 0.507714\n",
      "Cost after iteration 31900: 0.507160\n",
      "Cost after iteration 32000: 0.506542\n",
      "Cost after iteration 32100: 0.505939\n",
      "Cost after iteration 32200: 0.505337\n",
      "Cost after iteration 32300: 0.504625\n",
      "Cost after iteration 32400: 0.504048\n",
      "Cost after iteration 32500: 0.503415\n",
      "Cost after iteration 32600: 0.502770\n",
      "Cost after iteration 32700: 0.502126\n",
      "Cost after iteration 32800: 0.501313\n",
      "Cost after iteration 32900: 0.500654\n",
      "Cost after iteration 33000: 0.499998\n",
      "Cost after iteration 33100: 0.499338\n",
      "Cost after iteration 33200: 0.498649\n",
      "Cost after iteration 33300: 0.497728\n",
      "Cost after iteration 33400: 0.496953\n",
      "Cost after iteration 33500: 0.496364\n",
      "Cost after iteration 33600: 0.495607\n",
      "Cost after iteration 33700: 0.494832\n",
      "Cost after iteration 33800: 0.493971\n",
      "Cost after iteration 33900: 0.493277\n",
      "Cost after iteration 34000: 0.492586\n",
      "Cost after iteration 34100: 0.491840\n",
      "Cost after iteration 34200: 0.491148\n",
      "Cost after iteration 34300: 0.490324\n",
      "Cost after iteration 34400: 0.489541\n",
      "Cost after iteration 34500: 0.488873\n",
      "Cost after iteration 34600: 0.487979\n",
      "Cost after iteration 34700: 0.487323\n",
      "Cost after iteration 34800: 0.486448\n",
      "Cost after iteration 34900: 0.485789\n",
      "Cost after iteration 35000: 0.485033\n",
      "Cost after iteration 35100: 0.484167\n",
      "Cost after iteration 35200: 0.483342\n",
      "Cost after iteration 35300: 0.482559\n",
      "Cost after iteration 35400: 0.481747\n",
      "Cost after iteration 35500: 0.481107\n",
      "Cost after iteration 35600: 0.480243\n",
      "Cost after iteration 35700: 0.479449\n",
      "Cost after iteration 35800: 0.478658\n",
      "Cost after iteration 35900: 0.477854\n",
      "Cost after iteration 36000: 0.477199\n",
      "Cost after iteration 36100: 0.476259\n",
      "Cost after iteration 36200: 0.475576\n",
      "Cost after iteration 36300: 0.474838\n",
      "Cost after iteration 36400: 0.473965\n",
      "Cost after iteration 36500: 0.473249\n",
      "Cost after iteration 36600: 0.472280\n",
      "Cost after iteration 36700: 0.471512\n",
      "Cost after iteration 36800: 0.470770\n",
      "Cost after iteration 36900: 0.469969\n",
      "Cost after iteration 37000: 0.469284\n",
      "Cost after iteration 37100: 0.468381\n",
      "Cost after iteration 37200: 0.467594\n",
      "Cost after iteration 37300: 0.466955\n",
      "Cost after iteration 37400: 0.466006\n",
      "Cost after iteration 37500: 0.465247\n",
      "Cost after iteration 37600: 0.464464\n",
      "Cost after iteration 37700: 0.463680\n",
      "Cost after iteration 37800: 0.462952\n",
      "Cost after iteration 37900: 0.462047\n",
      "Cost after iteration 38000: 0.461248\n",
      "Cost after iteration 38100: 0.460596\n",
      "Cost after iteration 38200: 0.459822\n",
      "Cost after iteration 38300: 0.459014\n",
      "Cost after iteration 38400: 0.458158\n",
      "Cost after iteration 38500: 0.457401\n",
      "Cost after iteration 38600: 0.456457\n",
      "Cost after iteration 38700: 0.455825\n",
      "Cost after iteration 38800: 0.455069\n",
      "Cost after iteration 38900: 0.454254\n",
      "Cost after iteration 39000: 0.452786\n",
      "Cost after iteration 39100: 0.448410\n",
      "Cost after iteration 39200: 0.447553\n",
      "Cost after iteration 39300: 0.446511\n",
      "Cost after iteration 39400: 0.445377\n",
      "Cost after iteration 39500: 0.444228\n",
      "Cost after iteration 39600: 0.442878\n",
      "Cost after iteration 39700: 0.442029\n",
      "Cost after iteration 39800: 0.441335\n",
      "Cost after iteration 39900: 0.440367\n",
      "Cost after iteration 40000: 0.439562\n",
      "Cost after iteration 40100: 0.438678\n",
      "Cost after iteration 40200: 0.438036\n",
      "Cost after iteration 40300: 0.437125\n",
      "Cost after iteration 40400: 0.436392\n",
      "Cost after iteration 40500: 0.435506\n",
      "Cost after iteration 40600: 0.434871\n",
      "Cost after iteration 40700: 0.433980\n",
      "Cost after iteration 40800: 0.436456\n",
      "Cost after iteration 40900: 0.435138\n",
      "Cost after iteration 41000: 0.434364\n",
      "Cost after iteration 41100: 0.433636\n",
      "Cost after iteration 41200: 0.432691\n",
      "Cost after iteration 41300: 0.432312\n",
      "Cost after iteration 41400: 0.432055\n",
      "Cost after iteration 41500: 0.431006\n",
      "Cost after iteration 41600: 0.430566\n",
      "Cost after iteration 41700: 0.429571\n",
      "Cost after iteration 41800: 0.428944\n",
      "Cost after iteration 41900: 0.428270\n",
      "Cost after iteration 42000: 0.427124\n",
      "Cost after iteration 42100: 0.425985\n",
      "Cost after iteration 42200: 0.425544\n",
      "Cost after iteration 42300: 0.424874\n",
      "Cost after iteration 42400: 0.424660\n",
      "Cost after iteration 42500: 0.423700\n",
      "Cost after iteration 42600: 0.423587\n",
      "Cost after iteration 42700: 0.422747\n",
      "Cost after iteration 42800: 0.421826\n",
      "Cost after iteration 42900: 0.423875\n",
      "Cost after iteration 43000: 0.428230\n",
      "Cost after iteration 43100: 0.428155\n",
      "Cost after iteration 43200: 0.427729\n",
      "Cost after iteration 43300: 0.428482\n",
      "Cost after iteration 43400: 0.428172\n",
      "Cost after iteration 43500: 0.428648\n",
      "Cost after iteration 43600: 0.428112\n",
      "Cost after iteration 43700: 0.427759\n",
      "Cost after iteration 43800: 0.427402\n",
      "Cost after iteration 43900: 0.427063\n",
      "Cost after iteration 44000: 0.426703\n",
      "Cost after iteration 44100: 0.426508\n",
      "Cost after iteration 44200: 0.426157\n",
      "Cost after iteration 44300: 0.425861\n",
      "Cost after iteration 44400: 0.425543\n",
      "Cost after iteration 44500: 0.425214\n",
      "Cost after iteration 44600: 0.424916\n",
      "Cost after iteration 44700: 0.424630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 44800: 0.424354\n",
      "Cost after iteration 44900: 0.424029\n",
      "Cost after iteration 45000: 0.423876\n",
      "Cost after iteration 45100: 0.423625\n",
      "Cost after iteration 45200: 0.423371\n",
      "Cost after iteration 45300: 0.423048\n",
      "Cost after iteration 45400: 0.422706\n",
      "Cost after iteration 45500: 0.422379\n",
      "Cost after iteration 45600: 0.422204\n",
      "Cost after iteration 45700: 0.421930\n",
      "Cost after iteration 45800: 0.421584\n",
      "Cost after iteration 45900: 0.421397\n",
      "Cost after iteration 46000: 0.421018\n",
      "Cost after iteration 46100: 0.420774\n",
      "Cost after iteration 46200: 0.420493\n",
      "Cost after iteration 46300: 0.420232\n",
      "Cost after iteration 46400: 0.419981\n",
      "Cost after iteration 46500: 0.419769\n",
      "Cost after iteration 46600: 0.419501\n",
      "Cost after iteration 46700: 0.419244\n",
      "Cost after iteration 46800: 0.419044\n",
      "Cost after iteration 46900: 0.418773\n",
      "Cost after iteration 47000: 0.418565\n",
      "Cost after iteration 47100: 0.418318\n",
      "Cost after iteration 47200: 0.418108\n",
      "Cost after iteration 47300: 0.417909\n",
      "Cost after iteration 47400: 0.417724\n",
      "Cost after iteration 47500: 0.417542\n",
      "Cost after iteration 47600: 0.417341\n",
      "Cost after iteration 47700: 0.417168\n",
      "Cost after iteration 47800: 0.416990\n",
      "Cost after iteration 47900: 0.416811\n",
      "Cost after iteration 48000: 0.416609\n",
      "Cost after iteration 48100: 0.416398\n",
      "Cost after iteration 48200: 0.416302\n",
      "Cost after iteration 48300: 0.416116\n",
      "Cost after iteration 48400: 0.415966\n",
      "Cost after iteration 48500: 0.415736\n",
      "Cost after iteration 48600: 0.415625\n",
      "Cost after iteration 48700: 0.415481\n",
      "Cost after iteration 48800: 0.415328\n",
      "Cost after iteration 48900: 0.398228\n",
      "Cost after iteration 49000: 0.394285\n",
      "Cost after iteration 49100: 0.394351\n",
      "Cost after iteration 49200: 0.393997\n",
      "Cost after iteration 49300: 0.393822\n",
      "Cost after iteration 49400: 0.393804\n",
      "Cost after iteration 49500: 0.393566\n",
      "Cost after iteration 49600: 0.393357\n",
      "Cost after iteration 49700: 0.393205\n",
      "Cost after iteration 49800: 0.393026\n",
      "Cost after iteration 49900: 0.392896\n",
      "Cost after iteration 50000: 0.392647\n",
      "Cost after iteration 50100: 0.392500\n",
      "Cost after iteration 50200: 0.392301\n",
      "Cost after iteration 50300: 0.392156\n",
      "Cost after iteration 50400: 0.392036\n",
      "Cost after iteration 50500: 0.391916\n",
      "Cost after iteration 50600: 0.391784\n",
      "Cost after iteration 50700: 0.391675\n",
      "Cost after iteration 50800: 0.391567\n",
      "Cost after iteration 50900: 0.391407\n",
      "Cost after iteration 51000: 0.391321\n",
      "Cost after iteration 51100: 0.391224\n",
      "Cost after iteration 51200: 0.391130\n",
      "Cost after iteration 51300: 0.391006\n",
      "Cost after iteration 51400: 0.390941\n",
      "Cost after iteration 51500: 0.390850\n",
      "Cost after iteration 51600: 0.390762\n",
      "Cost after iteration 51700: 0.390675\n",
      "Cost after iteration 51800: 0.390669\n",
      "Cost after iteration 51900: 0.390635\n",
      "Cost after iteration 52000: 0.390606\n",
      "Cost after iteration 52100: 0.391129\n",
      "Cost after iteration 52200: 0.391345\n",
      "Cost after iteration 52300: 0.391280\n",
      "Cost after iteration 52400: 0.391087\n",
      "Cost after iteration 52500: 0.391039\n",
      "Cost after iteration 52600: 0.390976\n",
      "Cost after iteration 52700: 0.390924\n",
      "Cost after iteration 52800: 0.390754\n",
      "Cost after iteration 52900: 0.390790\n",
      "Cost after iteration 53000: 0.391029\n",
      "Cost after iteration 53100: 0.391289\n",
      "Cost after iteration 53200: 0.391274\n",
      "Cost after iteration 53300: 0.391101\n",
      "Cost after iteration 53400: 0.391069\n",
      "Cost after iteration 53500: 0.391054\n",
      "Cost after iteration 53600: 0.391036\n",
      "Cost after iteration 53700: 0.391418\n",
      "Cost after iteration 53800: 0.391378\n",
      "Cost after iteration 53900: 0.391640\n",
      "Cost after iteration 54000: 0.391618\n",
      "Cost after iteration 54100: 0.391846\n",
      "Cost after iteration 54200: 0.391820\n",
      "Cost after iteration 54300: 0.392062\n",
      "Cost after iteration 54400: 0.392026\n",
      "Cost after iteration 54500: 0.391994\n",
      "Cost after iteration 54600: 0.391962\n",
      "Cost after iteration 54700: 0.391925\n",
      "Cost after iteration 54800: 0.391902\n",
      "Cost after iteration 54900: 0.391880\n",
      "Cost after iteration 55000: 0.391847\n",
      "Cost after iteration 55100: 0.391834\n",
      "Cost after iteration 55200: 0.391815\n",
      "Cost after iteration 55300: 0.391806\n",
      "Cost after iteration 55400: 0.391794\n",
      "Cost after iteration 55500: 0.391772\n",
      "Cost after iteration 55600: 0.391757\n",
      "Cost after iteration 55700: 0.391750\n",
      "Cost after iteration 55800: 0.391741\n",
      "Cost after iteration 55900: 0.391728\n",
      "Cost after iteration 56000: 0.391720\n",
      "Cost after iteration 56100: 0.391712\n",
      "Cost after iteration 56200: 0.391706\n",
      "Cost after iteration 56300: 0.391703\n",
      "Cost after iteration 56400: 0.391704\n",
      "Cost after iteration 56500: 0.391705\n",
      "Cost after iteration 56600: 0.391698\n",
      "Cost after iteration 56700: 0.391685\n",
      "Cost after iteration 56800: 0.391685\n",
      "Cost after iteration 56900: 0.391681\n",
      "Cost after iteration 57000: 0.391674\n",
      "Cost after iteration 57100: 0.391674\n",
      "Cost after iteration 57200: 0.391681\n",
      "Cost after iteration 57300: 0.391691\n",
      "Cost after iteration 57400: 0.391674\n",
      "Cost after iteration 57500: 0.391672\n",
      "Cost after iteration 57600: 0.391668\n",
      "Cost after iteration 57700: 0.391668\n",
      "Cost after iteration 57800: 0.391667\n",
      "Cost after iteration 57900: 0.391661\n",
      "Cost after iteration 58000: 0.391657\n",
      "Cost after iteration 58100: 0.391654\n",
      "Cost after iteration 58200: 0.391650\n",
      "Cost after iteration 58300: 0.391652\n",
      "Cost after iteration 58400: 0.391659\n",
      "Cost after iteration 58500: 0.391661\n",
      "Cost after iteration 58600: 0.391668\n",
      "Cost after iteration 58700: 0.391676\n",
      "Cost after iteration 58800: 0.391679\n",
      "Cost after iteration 58900: 0.391685\n",
      "Cost after iteration 59000: 0.391688\n",
      "Cost after iteration 59100: 0.391691\n",
      "Cost after iteration 59200: 0.391693\n",
      "Cost after iteration 59300: 0.391691\n",
      "Cost after iteration 59400: 0.391692\n",
      "Cost after iteration 59500: 0.391694\n",
      "Cost after iteration 59600: 0.391696\n",
      "Cost after iteration 59700: 0.391699\n",
      "Cost after iteration 59800: 0.391700\n",
      "Cost after iteration 59900: 0.391699\n",
      "Cost after iteration 60000: 0.391704\n",
      "Cost after iteration 60100: 0.391708\n",
      "Cost after iteration 60200: 0.391715\n",
      "Cost after iteration 60300: 0.391724\n",
      "Cost after iteration 60400: 0.391734\n",
      "Cost after iteration 60500: 0.391745\n",
      "Cost after iteration 60600: 0.391756\n",
      "Cost after iteration 60700: 0.391765\n",
      "Cost after iteration 60800: 0.391778\n",
      "Cost after iteration 60900: 0.393530\n",
      "Cost after iteration 61000: 0.393609\n",
      "Cost after iteration 61100: 0.393623\n",
      "Cost after iteration 61200: 0.393634\n",
      "Cost after iteration 61300: 0.393937\n",
      "Cost after iteration 61400: 0.393845\n",
      "Cost after iteration 61500: 0.393852\n",
      "Cost after iteration 61600: 0.393861\n",
      "Cost after iteration 61700: 0.393871\n",
      "Cost after iteration 61800: 0.393879\n",
      "Cost after iteration 61900: 0.393885\n",
      "Cost after iteration 62000: 0.393886\n",
      "Cost after iteration 62100: 0.394135\n",
      "Cost after iteration 62200: 0.394093\n",
      "Cost after iteration 62300: 0.394073\n",
      "Cost after iteration 62400: 0.394073\n",
      "Cost after iteration 62500: 0.394076\n",
      "Cost after iteration 62600: 0.394077\n",
      "Cost after iteration 62700: 0.394078\n",
      "Cost after iteration 62800: 0.394077\n",
      "Cost after iteration 62900: 0.394077\n",
      "Cost after iteration 63000: 0.394076\n",
      "Cost after iteration 63100: 0.395686\n",
      "Cost after iteration 63200: 0.396649\n",
      "Cost after iteration 63300: 0.398903\n",
      "Cost after iteration 63400: 0.398510\n",
      "Cost after iteration 63500: 0.398446\n",
      "Cost after iteration 63600: 0.399179\n",
      "Cost after iteration 63700: 0.399159\n",
      "Cost after iteration 63800: 0.399679\n",
      "Cost after iteration 63900: 0.399393\n",
      "Cost after iteration 64000: 0.399358\n",
      "Cost after iteration 64100: 0.399339\n",
      "Cost after iteration 64200: 0.399322\n",
      "Cost after iteration 64300: 0.399301\n",
      "Cost after iteration 64400: 0.399281\n",
      "Cost after iteration 64500: 0.399280\n",
      "Cost after iteration 64600: 0.399259\n",
      "Cost after iteration 64700: 0.399240\n",
      "Cost after iteration 64800: 0.399220\n",
      "Cost after iteration 64900: 0.399219\n",
      "Cost after iteration 65000: 0.399198\n",
      "Cost after iteration 65100: 0.399177\n",
      "Cost after iteration 65200: 0.399178\n",
      "Cost after iteration 65300: 0.399158\n",
      "Cost after iteration 65400: 0.399136\n",
      "Cost after iteration 65500: 0.399116\n",
      "Cost after iteration 65600: 0.399099\n",
      "Cost after iteration 65700: 0.399101\n",
      "Cost after iteration 65800: 0.399084\n",
      "Cost after iteration 65900: 0.399068\n",
      "Cost after iteration 66000: 0.399034\n",
      "Cost after iteration 66100: 0.399023\n",
      "Cost after iteration 66200: 0.398998\n",
      "Cost after iteration 66300: 0.398965\n",
      "Cost after iteration 66400: 0.398937\n",
      "Cost after iteration 66500: 0.398919\n",
      "Cost after iteration 66600: 0.398906\n",
      "Cost after iteration 66700: 0.398896\n",
      "Cost after iteration 66800: 0.398886\n",
      "Cost after iteration 66900: 0.398875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 67000: 0.398864\n",
      "Cost after iteration 67100: 0.398852\n",
      "Cost after iteration 67200: 0.398839\n",
      "Cost after iteration 67300: 0.398826\n",
      "Cost after iteration 67400: 0.398814\n",
      "Cost after iteration 67500: 0.398803\n",
      "Cost after iteration 67600: 0.398790\n",
      "Cost after iteration 67700: 0.398779\n",
      "Cost after iteration 67800: 0.398767\n",
      "Cost after iteration 67900: 0.398754\n",
      "Cost after iteration 68000: 0.398742\n",
      "Cost after iteration 68100: 0.398732\n",
      "Cost after iteration 68200: 0.398721\n",
      "Cost after iteration 68300: 0.398712\n",
      "Cost after iteration 68400: 0.398704\n",
      "Cost after iteration 68500: 0.398694\n",
      "Cost after iteration 68600: 0.398685\n",
      "Cost after iteration 68700: 0.398678\n",
      "Cost after iteration 68800: 0.398671\n",
      "Cost after iteration 68900: 0.398664\n",
      "Cost after iteration 69000: 0.398657\n",
      "Cost after iteration 69100: 0.398651\n",
      "Cost after iteration 69200: 0.398668\n",
      "Cost after iteration 69300: 0.398667\n",
      "Cost after iteration 69400: 0.398662\n",
      "Cost after iteration 69500: 0.398656\n",
      "Cost after iteration 69600: 0.398651\n",
      "Cost after iteration 69700: 0.398647\n",
      "Cost after iteration 69800: 0.398642\n",
      "Cost after iteration 69900: 0.398637\n",
      "Cost after iteration 70000: 0.398633\n",
      "Cost after iteration 70100: 0.398628\n",
      "Cost after iteration 70200: 0.398623\n",
      "Cost after iteration 70300: 0.398617\n",
      "Cost after iteration 70400: 0.398613\n",
      "Cost after iteration 70500: 0.398608\n",
      "Cost after iteration 70600: 0.398602\n",
      "Cost after iteration 70700: 0.398597\n",
      "Cost after iteration 70800: 0.398593\n",
      "Cost after iteration 70900: 0.398588\n",
      "Cost after iteration 71000: 0.398584\n",
      "Cost after iteration 71100: 0.398579\n",
      "Cost after iteration 71200: 0.398574\n",
      "Cost after iteration 71300: 0.398568\n",
      "Cost after iteration 71400: 0.398562\n",
      "Cost after iteration 71500: 0.398558\n",
      "Cost after iteration 71600: 0.398553\n",
      "Cost after iteration 71700: 0.398549\n",
      "Cost after iteration 71800: 0.398545\n",
      "Cost after iteration 71900: 0.398541\n",
      "Cost after iteration 72000: 0.398537\n",
      "Cost after iteration 72100: 0.398533\n",
      "Cost after iteration 72200: 0.398530\n",
      "Cost after iteration 72300: 0.398527\n",
      "Cost after iteration 72400: 0.398523\n",
      "Cost after iteration 72500: 0.398519\n",
      "Cost after iteration 72600: 0.398516\n",
      "Cost after iteration 72700: 0.398512\n",
      "Cost after iteration 72800: 0.398509\n",
      "Cost after iteration 72900: 0.398505\n",
      "Cost after iteration 73000: 0.398501\n",
      "Cost after iteration 73100: 0.398497\n",
      "Cost after iteration 73200: 0.398494\n",
      "Cost after iteration 73300: 0.398490\n",
      "Cost after iteration 73400: 0.398487\n",
      "Cost after iteration 73500: 0.398484\n",
      "Cost after iteration 73600: 0.398481\n",
      "Cost after iteration 73700: 0.398479\n",
      "Cost after iteration 73800: 0.398477\n",
      "Cost after iteration 73900: 0.398475\n",
      "Cost after iteration 74000: 0.398472\n",
      "Cost after iteration 74100: 0.398469\n",
      "Cost after iteration 74200: 0.398467\n",
      "Cost after iteration 74300: 0.398464\n",
      "Cost after iteration 74400: 0.398461\n",
      "Cost after iteration 74500: 0.398459\n",
      "Cost after iteration 74600: 0.398457\n",
      "Cost after iteration 74700: 0.398456\n",
      "Cost after iteration 74800: 0.398454\n",
      "Cost after iteration 74900: 0.398451\n",
      "Cost after iteration 75000: 0.398449\n",
      "Cost after iteration 75100: 0.398446\n",
      "Cost after iteration 75200: 0.398444\n",
      "Cost after iteration 75300: 0.398442\n",
      "Cost after iteration 75400: 0.398440\n",
      "Cost after iteration 75500: 0.398439\n",
      "Cost after iteration 75600: 0.398437\n",
      "Cost after iteration 75700: 0.398435\n",
      "Cost after iteration 75800: 0.398434\n",
      "Cost after iteration 75900: 0.398431\n",
      "Cost after iteration 76000: 0.398429\n",
      "Cost after iteration 76100: 0.398427\n",
      "Cost after iteration 76200: 0.398425\n",
      "Cost after iteration 76300: 0.398423\n",
      "Cost after iteration 76400: 0.398421\n",
      "Cost after iteration 76500: 0.398419\n",
      "Cost after iteration 76600: 0.398417\n",
      "Cost after iteration 76700: 0.398414\n",
      "Cost after iteration 76800: 0.398412\n",
      "Cost after iteration 76900: 0.398409\n",
      "Cost after iteration 77000: 0.398406\n",
      "Cost after iteration 77100: 0.398404\n",
      "Cost after iteration 77200: 0.398401\n",
      "Cost after iteration 77300: 0.398399\n",
      "Cost after iteration 77400: 0.398396\n",
      "Cost after iteration 77500: 0.398394\n",
      "Cost after iteration 77600: 0.398391\n",
      "Cost after iteration 77700: 0.398389\n",
      "Cost after iteration 77800: 0.398387\n",
      "Cost after iteration 77900: 0.398385\n",
      "Cost after iteration 78000: 0.398383\n",
      "Cost after iteration 78100: 0.398381\n",
      "Cost after iteration 78200: 0.398379\n",
      "Cost after iteration 78300: 0.398377\n",
      "Cost after iteration 78400: 0.398375\n",
      "Cost after iteration 78500: 0.398373\n",
      "Cost after iteration 78600: 0.398371\n",
      "Cost after iteration 78700: 0.398369\n",
      "Cost after iteration 78800: 0.398368\n",
      "Cost after iteration 78900: 0.398366\n",
      "Cost after iteration 79000: 0.398364\n",
      "Cost after iteration 79100: 0.398362\n",
      "Cost after iteration 79200: 0.398360\n",
      "Cost after iteration 79300: 0.398358\n",
      "Cost after iteration 79400: 0.398356\n",
      "Cost after iteration 79500: 0.398355\n",
      "Cost after iteration 79600: 0.398353\n",
      "Cost after iteration 79700: 0.398351\n",
      "Cost after iteration 79800: 0.398350\n",
      "Cost after iteration 79900: 0.398348\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_dims = [25,3,2,1] #  4-layer model\n",
    "#[25,20, 7, 5, 1] & 0.0075 learning rate & 10k iteration = 83/79%\n",
    "# [25, 40, 30, 20, 1] & 1.0 learning rate 10k iterations = 77/79%\n",
    "# [25, 40, 30, 20, 1] 0.001, 20k iterations = 83/79%\n",
    "# [25, 20, 10, 1] & 0.001, 10k iterations = 83/79%\n",
    "# [25,3,2,1] & 0.1, 80k iterations = 89/86%\n",
    "# [25,3,2,1] & 0.01, 80k iterations = 87/84%\n",
    "# [25,7,3,1], 0.1 & 60k iterations = 76/79%\n",
    "# [25,4,3,2,1] is bad\n",
    "# [25,3,1], 80k iterations and 0.1 learning rate = 84/82\n",
    "#parameters = initialize_parameters_deep(layers_dims)\n",
    "# print(\"W1 = \" + str(parameters[\"W1\"].shape))\n",
    "# print(\"b1 = \" + str(parameters[\"b1\"].shape))\n",
    "# print(\"W2 = \" + str(parameters[\"W2\"].shape))\n",
    "# print(\"b2 = \" + str(parameters[\"b2\"].shape))\n",
    "# print(\"W3 = \" + str(parameters[\"W3\"].shape))\n",
    "# print(\"b3 = \" + str(parameters[\"b3\"].shape))\n",
    "# print(\"W4 = \" + str(parameters[\"W4\"].shape))\n",
    "# print(\"b4 = \" + str(parameters[\"b4\"].shape))\n",
    "# Z = np.dot(parameters[\"W1\"], X_train)+parameters[\"b1\"]\n",
    "# print(X_train.shape)\n",
    "# print(Z.shape)\n",
    "#assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "\n",
    "# AL, caches = L_model_forward(X_train, parameters)\n",
    "# print(AL.shape)\n",
    "# print(Y_train.shape)\n",
    "parameters = L_layer_model(X_train, Y_train, layers_dims, num_iterations = 80000,learning_rate=0.001, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = L_model_forward(X, parameters)\n",
    "    predictions = np.where(A2 > 0.5, 1, 0)  \n",
    "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
    "    return A2, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy for 83.38610935191049 %\n"
     ]
    }
   ],
   "source": [
    "probs, accuracy = predict(X_train, Y_train, parameters)\n",
    "print (\"Train Accuracy for {} %\".format(accuracy))\n",
    "# np.squeeze(probs)\n",
    "# predictions = np.where(probs > 0.6, 1, 0)  \n",
    "# accuracy = float((np.dot(Y_train,predictions.T) + np.dot(1-Y_train,1-predictions.T))/float(Y_train.size)*100)\n",
    "# print(accuracy)\n",
    "#print (\"Test Accuracy for {} %\".format(predict(X_test, Y_test, parameters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for 79.32489451476793 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.95      0.89       938\n",
      "         1.0       0.62      0.31      0.42       247\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1185\n",
      "   macro avg       0.73      0.63      0.65      1185\n",
      "weighted avg       0.79      0.82      0.79      1185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probs, accuracy = predict(X_test, Y_test, parameters)\n",
    "print (\"Test Accuracy for {} %\".format(accuracy))\n",
    "\n",
    "predictions = np.where(probs > 0.6, 1, 0)  \n",
    "# accuracy = float((np.dot(Y_test,predictions.T) + np.dot(1-Y_test,1-predictions.T))/float(Y_test.size)*100)\n",
    "# print(accuracy)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test[0], predictions[0]))\n",
    "#print(Y_test[0][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[779 159]\n",
      " [ 86 161]]\n",
      "0.6518218623481782\n",
      "0.503125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "predictions = np.where(probs > 0.5, 1, 0)\n",
    "print(confusion_matrix(Y_test[0], predictions[0]))\n",
    "print(recall_score(Y_test.astype(int)[0], predictions[0]))\n",
    "print(precision_score(Y_test.astype(int)[0], predictions[0]))\n",
    "\n",
    "# confusion_matrix(Y_test, np.where(probs > 0.6, 1, 0))\n",
    "# print (recall_score(y_test, np.where(probs > 0.6, 1, 0)))\n",
    "# print(precision_score(y_test, np.where(probs > 0.6, 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs230-env",
   "language": "python",
   "name": "cs230-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
