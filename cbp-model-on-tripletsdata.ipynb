{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = open(\"/Users/surthi/Documents/cs230project/data/nontriplets/part-00000-2028494a-080e-4dd9-b6d1-1dcc3cc31bf2.json\", \"r\").read()\n",
    "tjson_data = [json.loads(str(item)) for item in contents.strip().split('\\n')]\n",
    "tdf = json_normalize(tjson_data)\n",
    "\n",
    "tdf = tdf[['articleId', 'companyId', 'mentionMatched', 'articleLength', 'countOfDistinctIndustry', 'countOfOccurence', 'countOfOccurenceOfAliases', \n",
    "      'countOfOccurenceOfNamePreserveList', 'countOfSignificantStrings', 'countOfUrlMatch', \n",
    "      'countryMatchByOverlapScore', 'directCityMatchScore', 'directCountryMatchScore', 'directStateMatchScore',\n",
    "     'exactMatch', 'fieldMatch', 'isCommon', 'isIndustry', 'isProminent',\n",
    " 'isProminentByItself', 'isProminentByParent',  'isSingleCompany', 'isTickerMatch', \n",
    "      'isUrlMatched', 'personScore', 'stateMatchByOverlapScore', 'sumOfIndustry', 'w2vScore','isGold' ]].replace(-1000.0, np.nan)\n",
    "# convert boolean features to 0's and 1's:\n",
    "tdf[['isCommon', 'isIndustry', 'isProminent', 'fieldMatch',\n",
    " 'isProminentByItself', 'isProminentByParent',  'isSingleCompany', 'isTickerMatch', \n",
    "      'isUrlMatched', 'isGold','exactMatch']] *= 1.0\n",
    "\n",
    "for col in ['articleLength','countOfDistinctIndustry', 'countOfOccurence', 'countOfOccurenceOfAliases', \n",
    "      'countOfOccurenceOfNamePreserveList', 'countOfSignificantStrings', 'countOfUrlMatch', \n",
    "      'countryMatchByOverlapScore', 'directCityMatchScore', 'directCountryMatchScore', 'directStateMatchScore',\n",
    "     'exactMatch', 'fieldMatch', 'personScore', 'stateMatchByOverlapScore', 'sumOfIndustry', 'w2vScore']:\n",
    "    tdf[col] = norm(tdf[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = open(\"/Users/surthi/Documents/cs230project/data/1800rows/part-00000-27f46dc3-6c96-42d0-9e40-e22c729258d8.json\", \"r\").read() \n",
    "json_data = [json.loads(str(item)) for item in contents.strip().split('\\n')]\n",
    "df = json_normalize(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['articleId', 'articleLength', 'companyId',\n",
       "       'countOfDistinctIndustry', 'countOfOccurence',\n",
       "       'countOfOccurenceOfAliases', 'countOfOccurenceOfNamePreserveList',\n",
       "       'countOfSignificantStrings', 'countOfUrlMatch',\n",
       "       'countryMatchByOverlapScore', 'directCityMatchScore',\n",
       "       'directCountryMatchScore', 'directStateMatchScore', 'exactMatch',\n",
       "       'fieldMatch', 'isCommon', 'isGold', 'isIndustry', 'isProminent',\n",
       "       'isProminentByItself', 'isProminentByParent', 'isSingleCompany',\n",
       "       'isTickerMatch', 'isUrlMatched', 'mentionMatched', 'personScore',\n",
       "       'stateMatchByOverlapScore', 'sumOfIndustry', 'w2vScore'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['articleId', 'companyId', 'mentionMatched', 'articleLength', 'countOfDistinctIndustry', 'countOfOccurence', 'countOfOccurenceOfAliases', \n",
    "      'countOfOccurenceOfNamePreserveList', 'countOfSignificantStrings', 'countOfUrlMatch', \n",
    "      'countryMatchByOverlapScore', 'directCityMatchScore', 'directCountryMatchScore', 'directStateMatchScore',\n",
    "     'exactMatch', 'fieldMatch', 'isCommon', 'isIndustry', 'isProminent',\n",
    " 'isProminentByItself', 'isProminentByParent',  'isSingleCompany', 'isTickerMatch', \n",
    "      'isUrlMatched', 'personScore', 'stateMatchByOverlapScore', 'sumOfIndustry', 'w2vScore','isGold' ]].replace(-1000.0, np.nan)\n",
    "# convert boolean features to 0's and 1's:\n",
    "df[['isCommon', 'isIndustry', 'isProminent', 'fieldMatch',\n",
    " 'isProminentByItself', 'isProminentByParent',  'isSingleCompany', 'isTickerMatch', \n",
    "      'isUrlMatched', 'isGold','exactMatch']] *= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(X):\n",
    "    return (X - X.mean())/X.std()\n",
    "\n",
    "for col in ['articleLength','countOfDistinctIndustry', 'countOfOccurence', 'countOfOccurenceOfAliases', \n",
    "      'countOfOccurenceOfNamePreserveList', 'countOfSignificantStrings', 'countOfUrlMatch', \n",
    "      'countryMatchByOverlapScore', 'directCityMatchScore', 'directCountryMatchScore', 'directStateMatchScore',\n",
    "     'exactMatch', 'fieldMatch', 'personScore', 'stateMatchByOverlapScore', 'sumOfIndustry', 'w2vScore']:\n",
    "    df1[col] = norm(df1[col])\n",
    "    \n",
    "# for col in ['countOfDistinctIndustry', 'countOfOccurence', 'countOfOccurenceOfAliases', \n",
    "#       'countOfOccurenceOfNamePreserveList', 'countOfSignificantStrings', 'countOfUrlMatch', \n",
    "#       'countryMatchByOverlapScore', 'directCityMatchScore', 'directCountryMatchScore', 'directStateMatchScore',\n",
    "#      'exactMatch', 'fieldMatch', 'personScore', 'stateMatchByOverlapScore', 'sumOfIndustry', 'w2vScore']:\n",
    "#     df1[col] = norm(df1[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleLength</th>\n",
       "      <th>countOfDistinctIndustry</th>\n",
       "      <th>countOfOccurence</th>\n",
       "      <th>countOfOccurenceOfAliases</th>\n",
       "      <th>countOfOccurenceOfNamePreserveList</th>\n",
       "      <th>countOfSignificantStrings</th>\n",
       "      <th>countOfUrlMatch</th>\n",
       "      <th>countryMatchByOverlapScore</th>\n",
       "      <th>directCityMatchScore</th>\n",
       "      <th>directCountryMatchScore</th>\n",
       "      <th>...</th>\n",
       "      <th>isProminentByItself</th>\n",
       "      <th>isProminentByParent</th>\n",
       "      <th>isSingleCompany</th>\n",
       "      <th>isTickerMatch</th>\n",
       "      <th>isUrlMatched</th>\n",
       "      <th>personScore</th>\n",
       "      <th>stateMatchByOverlapScore</th>\n",
       "      <th>sumOfIndustry</th>\n",
       "      <th>w2vScore</th>\n",
       "      <th>isGold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.399000e+03</td>\n",
       "      <td>2.399000e+03</td>\n",
       "      <td>2.399000e+03</td>\n",
       "      <td>2.399000e+03</td>\n",
       "      <td>2.399000e+03</td>\n",
       "      <td>2.399000e+03</td>\n",
       "      <td>2.399000e+03</td>\n",
       "      <td>2.342000e+03</td>\n",
       "      <td>1.747000e+03</td>\n",
       "      <td>2.342000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>2399.000000</td>\n",
       "      <td>2399.000000</td>\n",
       "      <td>2399.0</td>\n",
       "      <td>2399.000000</td>\n",
       "      <td>2399.000000</td>\n",
       "      <td>960.000000</td>\n",
       "      <td>1.510000e+03</td>\n",
       "      <td>2.399000e+03</td>\n",
       "      <td>2309.000000</td>\n",
       "      <td>2399.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-9.477852e-17</td>\n",
       "      <td>2.369463e-17</td>\n",
       "      <td>-2.369463e-17</td>\n",
       "      <td>5.923658e-18</td>\n",
       "      <td>5.923658e-18</td>\n",
       "      <td>-8.885486e-18</td>\n",
       "      <td>1.777097e-17</td>\n",
       "      <td>-9.101743e-18</td>\n",
       "      <td>1.016804e-17</td>\n",
       "      <td>-2.427131e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198833</td>\n",
       "      <td>0.077532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.336390</td>\n",
       "      <td>0.037932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.764465e-17</td>\n",
       "      <td>2.961829e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399205</td>\n",
       "      <td>0.267490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472573</td>\n",
       "      <td>0.191073</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.642419e+00</td>\n",
       "      <td>-6.792327e-01</td>\n",
       "      <td>-7.198138e-01</td>\n",
       "      <td>-7.050452e-01</td>\n",
       "      <td>-6.931230e-01</td>\n",
       "      <td>-7.538503e-01</td>\n",
       "      <td>-1.663095e-01</td>\n",
       "      <td>-1.185912e+00</td>\n",
       "      <td>-2.515206e-01</td>\n",
       "      <td>-5.604636e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.516667</td>\n",
       "      <td>-8.773327e-01</td>\n",
       "      <td>-5.960521e-01</td>\n",
       "      <td>-0.535950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.209553e-01</td>\n",
       "      <td>-6.044482e-01</td>\n",
       "      <td>-4.637211e-01</td>\n",
       "      <td>-4.723580e-01</td>\n",
       "      <td>-4.830671e-01</td>\n",
       "      <td>-5.616217e-01</td>\n",
       "      <td>-1.663095e-01</td>\n",
       "      <td>-1.185912e+00</td>\n",
       "      <td>-2.515206e-01</td>\n",
       "      <td>-5.604636e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.516667</td>\n",
       "      <td>-8.773327e-01</td>\n",
       "      <td>-5.750212e-01</td>\n",
       "      <td>-0.535950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.406368e-01</td>\n",
       "      <td>-3.800948e-01</td>\n",
       "      <td>-4.637211e-01</td>\n",
       "      <td>-4.723580e-01</td>\n",
       "      <td>-4.830671e-01</td>\n",
       "      <td>-5.616217e-01</td>\n",
       "      <td>-1.663095e-01</td>\n",
       "      <td>8.428726e-01</td>\n",
       "      <td>-2.515206e-01</td>\n",
       "      <td>-5.604636e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.516667</td>\n",
       "      <td>-8.773327e-01</td>\n",
       "      <td>-4.067745e-01</td>\n",
       "      <td>-0.535950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.352700e-01</td>\n",
       "      <td>1.433967e-01</td>\n",
       "      <td>4.846440e-02</td>\n",
       "      <td>2.257037e-01</td>\n",
       "      <td>1.471004e-01</td>\n",
       "      <td>2.072928e-01</td>\n",
       "      <td>-1.663095e-01</td>\n",
       "      <td>8.428726e-01</td>\n",
       "      <td>-2.515206e-01</td>\n",
       "      <td>-5.604636e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.516667</td>\n",
       "      <td>1.139064e+00</td>\n",
       "      <td>1.384231e-02</td>\n",
       "      <td>0.154653</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.984998e+00</td>\n",
       "      <td>4.630466e+00</td>\n",
       "      <td>5.682504e+00</td>\n",
       "      <td>8.137069e+00</td>\n",
       "      <td>8.969446e+00</td>\n",
       "      <td>7.127524e+00</td>\n",
       "      <td>1.896270e+01</td>\n",
       "      <td>8.428726e-01</td>\n",
       "      <td>3.973542e+00</td>\n",
       "      <td>1.783475e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.058620</td>\n",
       "      <td>1.139064e+00</td>\n",
       "      <td>6.091755e+00</td>\n",
       "      <td>7.071088</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       articleLength  countOfDistinctIndustry  countOfOccurence  \\\n",
       "count   2.399000e+03             2.399000e+03      2.399000e+03   \n",
       "mean   -9.477852e-17             2.369463e-17     -2.369463e-17   \n",
       "std     1.000000e+00             1.000000e+00      1.000000e+00   \n",
       "min    -1.642419e+00            -6.792327e-01     -7.198138e-01   \n",
       "25%    -6.209553e-01            -6.044482e-01     -4.637211e-01   \n",
       "50%    -1.406368e-01            -3.800948e-01     -4.637211e-01   \n",
       "75%     5.352700e-01             1.433967e-01      4.846440e-02   \n",
       "max     5.984998e+00             4.630466e+00      5.682504e+00   \n",
       "\n",
       "       countOfOccurenceOfAliases  countOfOccurenceOfNamePreserveList  \\\n",
       "count               2.399000e+03                        2.399000e+03   \n",
       "mean                5.923658e-18                        5.923658e-18   \n",
       "std                 1.000000e+00                        1.000000e+00   \n",
       "min                -7.050452e-01                       -6.931230e-01   \n",
       "25%                -4.723580e-01                       -4.830671e-01   \n",
       "50%                -4.723580e-01                       -4.830671e-01   \n",
       "75%                 2.257037e-01                        1.471004e-01   \n",
       "max                 8.137069e+00                        8.969446e+00   \n",
       "\n",
       "       countOfSignificantStrings  countOfUrlMatch  countryMatchByOverlapScore  \\\n",
       "count               2.399000e+03     2.399000e+03                2.342000e+03   \n",
       "mean               -8.885486e-18     1.777097e-17               -9.101743e-18   \n",
       "std                 1.000000e+00     1.000000e+00                1.000000e+00   \n",
       "min                -7.538503e-01    -1.663095e-01               -1.185912e+00   \n",
       "25%                -5.616217e-01    -1.663095e-01               -1.185912e+00   \n",
       "50%                -5.616217e-01    -1.663095e-01                8.428726e-01   \n",
       "75%                 2.072928e-01    -1.663095e-01                8.428726e-01   \n",
       "max                 7.127524e+00     1.896270e+01                8.428726e-01   \n",
       "\n",
       "       directCityMatchScore  directCountryMatchScore  ...  \\\n",
       "count          1.747000e+03             2.342000e+03  ...   \n",
       "mean           1.016804e-17            -2.427131e-17  ...   \n",
       "std            1.000000e+00             1.000000e+00  ...   \n",
       "min           -2.515206e-01            -5.604636e-01  ...   \n",
       "25%           -2.515206e-01            -5.604636e-01  ...   \n",
       "50%           -2.515206e-01            -5.604636e-01  ...   \n",
       "75%           -2.515206e-01            -5.604636e-01  ...   \n",
       "max            3.973542e+00             1.783475e+00  ...   \n",
       "\n",
       "       isProminentByItself  isProminentByParent  isSingleCompany  \\\n",
       "count          2399.000000          2399.000000           2399.0   \n",
       "mean              0.198833             0.077532              0.0   \n",
       "std               0.399205             0.267490              0.0   \n",
       "min               0.000000             0.000000              0.0   \n",
       "25%               0.000000             0.000000              0.0   \n",
       "50%               0.000000             0.000000              0.0   \n",
       "75%               0.000000             0.000000              0.0   \n",
       "max               1.000000             1.000000              0.0   \n",
       "\n",
       "       isTickerMatch  isUrlMatched  personScore  stateMatchByOverlapScore  \\\n",
       "count    2399.000000   2399.000000   960.000000              1.510000e+03   \n",
       "mean        0.336390      0.037932     0.000000              3.764465e-17   \n",
       "std         0.472573      0.191073     1.000000              1.000000e+00   \n",
       "min         0.000000      0.000000    -0.516667             -8.773327e-01   \n",
       "25%         0.000000      0.000000    -0.516667             -8.773327e-01   \n",
       "50%         0.000000      0.000000    -0.516667             -8.773327e-01   \n",
       "75%         1.000000      0.000000    -0.516667              1.139064e+00   \n",
       "max         1.000000      1.000000     2.058620              1.139064e+00   \n",
       "\n",
       "       sumOfIndustry     w2vScore       isGold  \n",
       "count   2.399000e+03  2309.000000  2399.000000  \n",
       "mean    2.961829e-17     0.000000     0.230930  \n",
       "std     1.000000e+00     1.000000     0.421515  \n",
       "min    -5.960521e-01    -0.535950     0.000000  \n",
       "25%    -5.750212e-01    -0.535950     0.000000  \n",
       "50%    -4.067745e-01    -0.535950     0.000000  \n",
       "75%     1.384231e-02     0.154653     0.000000  \n",
       "max     6.091755e+00     7.071088     1.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df1 = df1.replace(np.nan, 0.0)\n",
    "np.random.seed(1)\n",
    "train, test = train_test_split(df1, test_size=0.2)\n",
    "x_test = test.loc[:, (test.columns != 'isGold') & (test.columns != 'articleId') & (test.columns != 'mentionMatched') & (test.columns != 'companyId')]#.T.values  & test.columns != 'articleId' & test.columns !='companyId' & test.columns != 'mentionMatched'\n",
    "y_test = test.loc[:, test.columns == 'isGold']#.T.values\n",
    "id_test = test.loc[:, (test.columns == 'articleId') | (test.columns == 'mentionMatched') | (test.columns == 'companyId')]\n",
    "x_train = train.loc[:, (train.columns != 'isGold')  & (train.columns != 'articleId') & (train.columns != 'mentionMatched') & (train.columns != 'companyId')]#.T.values\n",
    "y_train = train.loc[:, train.columns == 'isGold']#.T.values\n",
    "id_train = train.loc[:, (train.columns == 'articleId') | (train.columns == 'mentionMatched') | (train.columns == 'companyId')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1919/1919 [==============================] - 1s 648us/step - loss: 0.6697 - acc: 0.6769\n",
      "Epoch 2/100\n",
      "1919/1919 [==============================] - 0s 28us/step - loss: 0.5785 - acc: 0.7671\n",
      "Epoch 3/100\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 0.5398 - acc: 0.7665\n",
      "Epoch 4/100\n",
      "1919/1919 [==============================] - 0s 31us/step - loss: 0.5200 - acc: 0.7665\n",
      "Epoch 5/100\n",
      "1919/1919 [==============================] - 0s 26us/step - loss: 0.4996 - acc: 0.7718\n",
      "Epoch 6/100\n",
      "1919/1919 [==============================] - 0s 31us/step - loss: 0.4740 - acc: 0.7863\n",
      "Epoch 7/100\n",
      "1919/1919 [==============================] - 0s 28us/step - loss: 0.4430 - acc: 0.8072\n",
      "Epoch 8/100\n",
      "1919/1919 [==============================] - 0s 27us/step - loss: 0.4087 - acc: 0.8348\n",
      "Epoch 9/100\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 0.3750 - acc: 0.8557\n",
      "Epoch 10/100\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 0.3443 - acc: 0.8697\n",
      "Epoch 11/100\n",
      "1919/1919 [==============================] - 0s 29us/step - loss: 0.3188 - acc: 0.8822\n",
      "Epoch 12/100\n",
      "1919/1919 [==============================] - 0s 26us/step - loss: 0.2978 - acc: 0.8979\n",
      "Epoch 13/100\n",
      "1919/1919 [==============================] - 0s 27us/step - loss: 0.2809 - acc: 0.9026\n",
      "Epoch 14/100\n",
      "1919/1919 [==============================] - 0s 26us/step - loss: 0.2686 - acc: 0.9109\n",
      "Epoch 15/100\n",
      "1919/1919 [==============================] - 0s 27us/step - loss: 0.2594 - acc: 0.9145\n",
      "Epoch 16/100\n",
      "1919/1919 [==============================] - 0s 31us/step - loss: 0.2525 - acc: 0.9135\n",
      "Epoch 17/100\n",
      "1919/1919 [==============================] - 0s 35us/step - loss: 0.2474 - acc: 0.9171\n",
      "Epoch 18/100\n",
      "1919/1919 [==============================] - 0s 42us/step - loss: 0.2435 - acc: 0.9156\n",
      "Epoch 19/100\n",
      "1919/1919 [==============================] - 0s 35us/step - loss: 0.2416 - acc: 0.9182\n",
      "Epoch 20/100\n",
      "1919/1919 [==============================] - 0s 32us/step - loss: 0.2390 - acc: 0.9177\n",
      "Epoch 21/100\n",
      "1919/1919 [==============================] - 0s 31us/step - loss: 0.2373 - acc: 0.9182\n",
      "Epoch 22/100\n",
      "1919/1919 [==============================] - 0s 34us/step - loss: 0.2357 - acc: 0.9187\n",
      "Epoch 23/100\n",
      "1919/1919 [==============================] - 0s 38us/step - loss: 0.2346 - acc: 0.9182\n",
      "Epoch 24/100\n",
      "1919/1919 [==============================] - 0s 32us/step - loss: 0.2340 - acc: 0.9171\n",
      "Epoch 25/100\n",
      "1919/1919 [==============================] - 0s 28us/step - loss: 0.2331 - acc: 0.9187\n",
      "Epoch 26/100\n",
      "1919/1919 [==============================] - 0s 23us/step - loss: 0.2327 - acc: 0.9203\n",
      "Epoch 27/100\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 0.2328 - acc: 0.9203\n",
      "Epoch 28/100\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 0.2320 - acc: 0.9208\n",
      "Epoch 29/100\n",
      "1919/1919 [==============================] - 0s 26us/step - loss: 0.2313 - acc: 0.9203\n",
      "Epoch 30/100\n",
      "1919/1919 [==============================] - 0s 46us/step - loss: 0.2304 - acc: 0.9203\n",
      "Epoch 31/100\n",
      "1919/1919 [==============================] - 0s 42us/step - loss: 0.2302 - acc: 0.9203\n",
      "Epoch 32/100\n",
      "1919/1919 [==============================] - 0s 40us/step - loss: 0.2296 - acc: 0.9197\n",
      "Epoch 33/100\n",
      "1919/1919 [==============================] - 0s 42us/step - loss: 0.2292 - acc: 0.9197\n",
      "Epoch 34/100\n",
      "1919/1919 [==============================] - 0s 37us/step - loss: 0.2294 - acc: 0.9192\n",
      "Epoch 35/100\n",
      "1919/1919 [==============================] - 0s 37us/step - loss: 0.2290 - acc: 0.9187\n",
      "Epoch 36/100\n",
      "1919/1919 [==============================] - 0s 41us/step - loss: 0.2282 - acc: 0.9208\n",
      "Epoch 37/100\n",
      "1919/1919 [==============================] - 0s 35us/step - loss: 0.2287 - acc: 0.9192\n",
      "Epoch 38/100\n",
      "1919/1919 [==============================] - 0s 49us/step - loss: 0.2282 - acc: 0.9208\n",
      "Epoch 39/100\n",
      "1919/1919 [==============================] - 0s 51us/step - loss: 0.2275 - acc: 0.9187\n",
      "Epoch 40/100\n",
      "1919/1919 [==============================] - 0s 42us/step - loss: 0.2275 - acc: 0.9192\n",
      "Epoch 41/100\n",
      "1919/1919 [==============================] - 0s 38us/step - loss: 0.2280 - acc: 0.9203\n",
      "Epoch 42/100\n",
      "1919/1919 [==============================] - 0s 45us/step - loss: 0.2270 - acc: 0.9197\n",
      "Epoch 43/100\n",
      "1919/1919 [==============================] - 0s 28us/step - loss: 0.2264 - acc: 0.9187\n",
      "Epoch 44/100\n",
      "1919/1919 [==============================] - 0s 25us/step - loss: 0.2265 - acc: 0.9192\n",
      "Epoch 45/100\n",
      "1919/1919 [==============================] - 0s 26us/step - loss: 0.2264 - acc: 0.9187\n",
      "Epoch 46/100\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 0.2258 - acc: 0.9203\n",
      "Epoch 47/100\n",
      "1919/1919 [==============================] - 0s 29us/step - loss: 0.2259 - acc: 0.9203\n",
      "Epoch 48/100\n",
      "1919/1919 [==============================] - 0s 28us/step - loss: 0.2257 - acc: 0.9208\n",
      "Epoch 49/100\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 0.2260 - acc: 0.9208\n",
      "Epoch 50/100\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 0.2254 - acc: 0.9203\n",
      "Epoch 51/100\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 0.2254 - acc: 0.9197\n",
      "Epoch 52/100\n",
      "1919/1919 [==============================] - 0s 27us/step - loss: 0.2253 - acc: 0.9192\n",
      "Epoch 53/100\n",
      "1919/1919 [==============================] - 0s 38us/step - loss: 0.2249 - acc: 0.9213\n",
      "Epoch 54/100\n",
      "1919/1919 [==============================] - 0s 23us/step - loss: 0.2252 - acc: 0.9218\n",
      "Epoch 55/100\n",
      "1919/1919 [==============================] - 0s 23us/step - loss: 0.2247 - acc: 0.9203\n",
      "Epoch 56/100\n",
      "1919/1919 [==============================] - 0s 22us/step - loss: 0.2248 - acc: 0.9218\n",
      "Epoch 57/100\n",
      "1919/1919 [==============================] - 0s 23us/step - loss: 0.2249 - acc: 0.9208\n",
      "Epoch 58/100\n",
      "1919/1919 [==============================] - 0s 22us/step - loss: 0.2245 - acc: 0.9208\n",
      "Epoch 59/100\n",
      "1919/1919 [==============================] - 0s 22us/step - loss: 0.2247 - acc: 0.9197\n",
      "Epoch 60/100\n",
      "1919/1919 [==============================] - 0s 21us/step - loss: 0.2252 - acc: 0.9197\n",
      "Epoch 61/100\n",
      "1919/1919 [==============================] - 0s 20us/step - loss: 0.2242 - acc: 0.9203\n",
      "Epoch 62/100\n",
      "1919/1919 [==============================] - 0s 21us/step - loss: 0.2244 - acc: 0.9208\n",
      "Epoch 63/100\n",
      "1919/1919 [==============================] - 0s 20us/step - loss: 0.2246 - acc: 0.9197\n",
      "Epoch 64/100\n",
      "1919/1919 [==============================] - 0s 22us/step - loss: 0.2253 - acc: 0.9208\n",
      "Epoch 65/100\n",
      "1919/1919 [==============================] - 0s 21us/step - loss: 0.2244 - acc: 0.9192\n",
      "Epoch 66/100\n",
      "1919/1919 [==============================] - 0s 22us/step - loss: 0.2241 - acc: 0.9203\n",
      "Epoch 67/100\n",
      "1919/1919 [==============================] - 0s 21us/step - loss: 0.2242 - acc: 0.9197\n",
      "Epoch 68/100\n",
      "1919/1919 [==============================] - 0s 21us/step - loss: 0.2238 - acc: 0.9208\n",
      "Epoch 69/100\n",
      "1919/1919 [==============================] - 0s 21us/step - loss: 0.2247 - acc: 0.9203\n",
      "Epoch 70/100\n",
      "1919/1919 [==============================] - 0s 21us/step - loss: 0.2241 - acc: 0.9224\n",
      "Epoch 71/100\n",
      "1919/1919 [==============================] - 0s 22us/step - loss: 0.2235 - acc: 0.9224\n",
      "Epoch 72/100\n",
      "1919/1919 [==============================] - 0s 21us/step - loss: 0.2244 - acc: 0.9213\n",
      "Epoch 73/100\n",
      "1919/1919 [==============================] - 0s 36us/step - loss: 0.2238 - acc: 0.9203\n",
      "Epoch 74/100\n",
      "1919/1919 [==============================] - 0s 46us/step - loss: 0.2234 - acc: 0.9208\n",
      "Epoch 75/100\n",
      "1919/1919 [==============================] - 0s 40us/step - loss: 0.2237 - acc: 0.9203\n",
      "Epoch 76/100\n",
      "1919/1919 [==============================] - 0s 39us/step - loss: 0.2237 - acc: 0.9208\n",
      "Epoch 77/100\n",
      "1919/1919 [==============================] - 0s 40us/step - loss: 0.2236 - acc: 0.9197\n",
      "Epoch 78/100\n",
      "1919/1919 [==============================] - 0s 38us/step - loss: 0.2231 - acc: 0.9203\n",
      "Epoch 79/100\n",
      "1919/1919 [==============================] - 0s 41us/step - loss: 0.2236 - acc: 0.9213\n",
      "Epoch 80/100\n",
      "1919/1919 [==============================] - 0s 45us/step - loss: 0.2239 - acc: 0.9208\n",
      "Epoch 81/100\n",
      "1919/1919 [==============================] - 0s 45us/step - loss: 0.2237 - acc: 0.9218\n",
      "Epoch 82/100\n",
      "1919/1919 [==============================] - 0s 34us/step - loss: 0.2231 - acc: 0.9208\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1919/1919 [==============================] - 0s 35us/step - loss: 0.2232 - acc: 0.9213\n",
      "Epoch 84/100\n",
      "1919/1919 [==============================] - 0s 32us/step - loss: 0.2234 - acc: 0.9213\n",
      "Epoch 85/100\n",
      "1919/1919 [==============================] - 0s 26us/step - loss: 0.2236 - acc: 0.9203\n",
      "Epoch 86/100\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 0.2230 - acc: 0.9208\n",
      "Epoch 87/100\n",
      "1919/1919 [==============================] - 0s 36us/step - loss: 0.2227 - acc: 0.9213\n",
      "Epoch 88/100\n",
      "1919/1919 [==============================] - 0s 47us/step - loss: 0.2228 - acc: 0.9213\n",
      "Epoch 89/100\n",
      "1919/1919 [==============================] - 0s 46us/step - loss: 0.2229 - acc: 0.9218\n",
      "Epoch 90/100\n",
      "1919/1919 [==============================] - 0s 49us/step - loss: 0.2228 - acc: 0.9224\n",
      "Epoch 91/100\n",
      "1919/1919 [==============================] - 0s 45us/step - loss: 0.2268 - acc: 0.9213\n",
      "Epoch 92/100\n",
      "1919/1919 [==============================] - 0s 51us/step - loss: 0.2236 - acc: 0.9192\n",
      "Epoch 93/100\n",
      "1919/1919 [==============================] - 0s 45us/step - loss: 0.2225 - acc: 0.9208\n",
      "Epoch 94/100\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 0.2231 - acc: 0.9213\n",
      "Epoch 95/100\n",
      "1919/1919 [==============================] - 0s 43us/step - loss: 0.2231 - acc: 0.9208\n",
      "Epoch 96/100\n",
      "1919/1919 [==============================] - 0s 58us/step - loss: 0.2224 - acc: 0.9213\n",
      "Epoch 97/100\n",
      "1919/1919 [==============================] - 0s 61us/step - loss: 0.2225 - acc: 0.9208\n",
      "Epoch 98/100\n",
      "1919/1919 [==============================] - 0s 26us/step - loss: 0.2225 - acc: 0.9208\n",
      "Epoch 99/100\n",
      "1919/1919 [==============================] - 0s 27us/step - loss: 0.2226 - acc: 0.9229\n",
      "Epoch 100/100\n",
      "1919/1919 [==============================] - 0s 31us/step - loss: 0.2233 - acc: 0.9229\n",
      "480/480 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.backend import mean, sum\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, GRU, Concatenate, Multiply, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Lambda\n",
    "np.random.seed(1)\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=25, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "# model.add(Dense(15, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(10, activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=100)\n",
    "score = model.evaluate(x_test, y_test, batch_size=100)\n",
    "\n",
    "# model = Sequential([Dense(20, input_shape=(29,)), Activation('relu'), Dense(10), Activation('sigmoid')])\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.96      0.95       375\n",
      "         1.0       0.86      0.79      0.82       105\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       480\n",
      "   macro avg       0.90      0.88      0.89       480\n",
      "weighted avg       0.92      0.93      0.92       480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(x_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.asarray(y_test), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.93      0.91      2747\n",
      "         1.0       0.71      0.59      0.65       776\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      3523\n",
      "   macro avg       0.80      0.76      0.78      3523\n",
      "weighted avg       0.85      0.86      0.85      3523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tdf\n",
    "x_validation = tdf.loc[:, (tdf.columns != 'isGold') & (tdf.columns != 'articleId') & (tdf.columns != 'mentionMatched') & (tdf.columns != 'companyId')]\n",
    "y_validation = tdf.loc[:, tdf.columns == 'isGold']#.T.values\n",
    "\n",
    "y_validation_pred = model.predict_classes(x_validation)\n",
    "\n",
    "print(classification_report(np.asarray(y_validation), y_validation_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "output_path = \"1800rows_data_95_82_f1score_model.json\"\n",
    "weights_output_path = \"1800rows_data_95_82_f1score_model_weights.h5\"\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(output_path, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(weights_output_path)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "acc: 92.50%\n"
     ]
    }
   ],
   "source": [
    "# later...\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open(output_path, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(weights_output_path)\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs230-env",
   "language": "python",
   "name": "cs230-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
